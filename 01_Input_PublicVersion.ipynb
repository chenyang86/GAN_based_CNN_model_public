{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 0. basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import difflib\n",
    "import shapefile  \n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn import linear_model\n",
    "from sklearn import linear_model\n",
    "from random import sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def Loc(x,y,a,b):\n",
    "    # Enter two pairs of longitude and latitude to restore the horizontal and vertical distance\n",
    "    x1 = (a-x)/180*np.pi*meter_R\n",
    "    y1 = (b-y)/180*np.pi*meter_R \n",
    "    \n",
    "    return x1,y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataInDIR = # where your raw data stores\n",
    "dataSaveDIR = # where you want to save your derived data\n",
    "\n",
    "etyName2018 = # 2018 entry data file\n",
    "etyName2019 = # 2019 entry data file\n",
    "\n",
    "etylat = #entry latitude label name\n",
    "etylon = #entry longitude label name\n",
    "etyopendate = #entry openning date label name\n",
    "\n",
    "mile_R = 3963.191 # earth radius in miles\n",
    "meter_R = 6378137 # earth radius in meters\n",
    "\n",
    "# feature choosing range\n",
    "      # acs\n",
    "M_delta_radious = # Criteria for raw data, within 5 miles\n",
    "M_delta_degrees = math.degrees(M_delta_radious)\n",
    "\n",
    "    # ety-acs\n",
    "M_delta_radious2 = 5/mile_R \n",
    "M_delta_degrees2 = math.degrees(M_delta_radious2)\n",
    "\n",
    "    # ety feature\n",
    "M_delta_radious3 = 10/mile_R \n",
    "M_delta_degrees3 = math.degrees(M_delta_radious3)\n",
    "\n",
    "    # poi feature\n",
    "p_delta_radious = 5/mile_R \n",
    "p_delta_degrees = math.degrees(p_delta_radious)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in data\n",
    "ety2018 = pd.read_stata(dataInDIR+etyName2018)  \n",
    "ety2019 = pd.read_stata(dataInDIR+etyName2019)  \n",
    "\n",
    "ety2018 = ety2018.loc[:,['opening_placekey','opening_latitude','opening_longitude','opening_month']]\n",
    "ety2018.columns = ['sg_placekey', etylat,etylon,etyopendate]\n",
    "\n",
    "ety2019 = ety2019.loc[:,['opening_placekey','opening_lat','opening_lon','opening_mon']]\n",
    "ety2019.columns = ['sg_placekey', etylat,etylon,etyopendate]\n",
    "\n",
    "ety2018.insert(loc=0,column='No',value=range(len(ety2018.index)),allow_duplicates=True)\n",
    "ety2019.insert(loc=0,column='No',value=range(len(ety2019.index)),allow_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entryMon = ety2018[etyopendate].copy()\n",
    "etyMon = []\n",
    "\n",
    "for item in entryMon:\n",
    "\n",
    "    item = int(float(item))\n",
    "    if item<=12:\n",
    "        temp = datetime.datetime(2018, item, 1, 0, 0, 0)\n",
    "    elif item<=24:\n",
    "        temp = datetime.datetime(2019, item-12, 1, 0, 0, 0)\n",
    "\n",
    "    etyMon.append(temp)\n",
    "    \n",
    "ety2018[etyopendate] = etyMon\n",
    "\n",
    "entryMon = ety2019[etyopendate].copy()\n",
    "etyMon = []\n",
    "\n",
    "for item in entryMon:\n",
    "    item = int(float(item))\n",
    "    if item<=12:\n",
    "        temp = datetime.datetime(2018, item, 1, 0, 0, 0)\n",
    "    elif item<=24:\n",
    "        temp = datetime.datetime(2019, item-12, 1, 0, 0, 0)\n",
    "\n",
    "    etyMon.append(temp)\n",
    "ety2019[etyopendate] = etyMon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ety = ety2018.append(ety2019)\n",
    "ety['No'] = range(len(ety.index))\n",
    "ety.index = range(len(ety.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature ACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in data\n",
    "acs1 = pd.read_stata(dataInDIR+filenameACSdata)  \n",
    "\n",
    "# basic processing\n",
    "acs2 = acs1[['population','share_age_under16', 'share_age_16_64','share_white','share_college', 'share_collegeplus',\n",
    "            'median_hhinc','share_unemployed', 'median_hh_value','share_trans_car', 'share_trans_public', 'share_trans_home',  \n",
    "'share_travel_0_15', 'share_travel_15_30', 'share_travel_30_45', 'share_travel_45_60', 'share_travel_60_90', 'cbg_id'\n",
    " ]]\n",
    "minorRace = 1-acs1['share_white']\n",
    "acs2['minorRace'] = minorRace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# latitude and longitude\n",
    "# read in CBG data of USA\n",
    "state = 'US'\n",
    "fname =dataInDIR +'{}.txt'.format(state)\n",
    "dataST = pd.read_csv(fname)\n",
    "\n",
    "# add FIPS label\n",
    "lendataST = len(dataST.index)\n",
    "FIPSs = []\n",
    "for i in range(lendataST):\n",
    "    if i%5000==0:\n",
    "        print(i)\n",
    "    tractce1 = str(dataST.loc[i,'STATEFP'])\n",
    "    tractce1 = (2-len(tractce1))*'0'+tractce1\n",
    "    tractce2 = str(dataST.loc[i,'COUNTYFP'])\n",
    "    tractce2 = (3-len(tractce2))*'0'+tractce2\n",
    "    tractce3 = str(dataST.loc[i,'TRACTCE'])\n",
    "    tractce3 = (6-len(tractce3))*'0'+tractce3\n",
    "    tractce4 = str(dataST.loc[i,'BLKGRPCE'])\n",
    "    tractce4 = (1-len(tractce4))*'0'+tractce4\n",
    "    tractce = tractce1+tractce2+tractce3+tractce4\n",
    "    #print(tractce)\n",
    "    FIPSs.append(tractce)\n",
    "\n",
    "dataST['cbg_id'] = FIPSs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fusing block information to get latitude and longitude\n",
    "acs3 = pd.merge(acs2,dataST,on='cbg_id',how='left')\n",
    "\n",
    "# screen outliers\n",
    "acs4 = acs3.copy()\n",
    "acs4 = acs4.drop_duplicates(subset=['cbg_id']) # the amounts keeps the same after dropping duplication\n",
    "acs4 = acs4.dropna(axis = 0,how='any')\n",
    "acs4.index = range(len(acs4.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Delineate the grid scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter acs points based on entry perimeter\n",
    "\n",
    "acs5 = acs4.copy()\n",
    "cnt = 0\n",
    "cnt0 = 0\n",
    "for i in range(len(ety.index)):\n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "\n",
    "    temp = acs5[acs5['LATITUDE']>=lat-M_delta_degrees].copy()\n",
    "    temp = temp[temp['LATITUDE']<=lat+M_delta_degrees]\n",
    "    temp = temp[temp['LONGITUDE']>=lon-M_delta_degrees]\n",
    "    temp = temp[temp['LONGITUDE']<=lon+M_delta_degrees]\n",
    "    cnt+=len(temp.index)\n",
    "    \n",
    "    if len(temp.index)==0:\n",
    "        cnt0+=1\n",
    "        \n",
    "    if i==0:\n",
    "        outcome = temp\n",
    "    else:\n",
    "        outcome = outcome.append(temp)\n",
    "\n",
    "print('cnt',cnt,'entry with no surroundings',cnt0)\n",
    "outcome = outcome.drop_duplicates(subset=['cbg_id'])\n",
    "acs6 = outcome.copy()\n",
    "acs6.index = range(len(acs6.index))\n",
    "acs6.insert(loc=0,column='No',value=range(len(acs6.index)),allow_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid_S_S\n",
    "\n",
    "grid_S_S = []\n",
    "ety2 = ety.copy(deep=True)\n",
    "\n",
    "for i in range(len(ety.index)):\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "        \n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "    temp = ety2[ety2[etylat]>=lat-M_delta_degrees3].copy()\n",
    "    temp = temp[temp[etylat]<=lat+M_delta_degrees3]\n",
    "    temp = temp[temp[etylon]>=lon-M_delta_degrees3]\n",
    "    temp = temp[temp[etylon]<=lon+M_delta_degrees3]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,etylat],temp.loc[j,etylon])\n",
    "        line = [ety.loc[i,'No'],temp.loc[j,'No'],xt,yt]\n",
    "        grid_S_S.append(line)\n",
    "temp = np.array(grid_S_S)\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_oth','x','y'])\n",
    "\n",
    "# save data\n",
    "\n",
    "out.to_csv(dataSaveDIR+'grid_S_S.csv'\n",
    "           ,index=False,header=out.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid_S_I\n",
    "grid_S_I = []\n",
    "for i in range(len(ety.index)):\n",
    "    print(i)\n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "    temp = acs6[acs6['LATITUDE']>=lat-M_delta_degrees2].copy()\n",
    "    temp = temp[temp['LATITUDE']<=lat+M_delta_degrees2]\n",
    "    temp = temp[temp['LONGITUDE']>=lon-M_delta_degrees2]\n",
    "    temp = temp[temp['LONGITUDE']<=lon+M_delta_degrees2]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'LATITUDE'],temp.loc[j,'LONGITUDE'])\n",
    "        line = [ety.loc[i,'No'],temp.loc[j,'No'],xt,yt,temp.loc[j,'population']\n",
    "                                                        ,temp.loc[j,'share_age_under16']\n",
    "                                                        ,temp.loc[j,'share_age_16_64']\n",
    "                                                        ,temp.loc[j,'share_white']\n",
    "                                                        ,temp.loc[j,'share_college']\n",
    "                                                        ,temp.loc[j,'share_collegeplus']\n",
    "                                                        ,temp.loc[j,'median_hhinc']\n",
    "                                                        ,temp.loc[j,'share_unemployed']\n",
    "                                                        ,temp.loc[j,'median_hh_value']\n",
    "                                                        ,temp.loc[j,'share_trans_car']\n",
    "                                                        ,temp.loc[j,'share_trans_public']\n",
    "                                                        ,temp.loc[j,'share_trans_home']\n",
    "                                                        ,temp.loc[j,'share_travel_0_15']\n",
    "                                                        ,temp.loc[j,'share_travel_15_30']\n",
    "                                                        ,temp.loc[j,'share_travel_30_45']\n",
    "                                                        ,temp.loc[j,'share_travel_45_60']\n",
    "                                                        ,temp.loc[j,'share_travel_60_90']\n",
    "                                                        ,temp.loc[j,'cbg_id']\n",
    "                                                        ,temp.loc[j,'minorRace']]\n",
    "        grid_S_I.append(line)\n",
    "temp = np.array(grid_S_I)\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_oth','x','y','population', 'share_age_under16', 'share_age_16_64',\n",
    "       'share_white', 'share_college', 'share_collegeplus', 'median_hhinc',\n",
    "       'share_unemployed', 'median_hh_value', 'share_trans_car',\n",
    "       'share_trans_public', 'share_trans_home', 'share_travel_0_15',\n",
    "       'share_travel_15_30', 'share_travel_30_45', 'share_travel_45_60',\n",
    "       'share_travel_60_90', 'cbg_id', 'minorRace'])\n",
    "\n",
    "# save data\n",
    "\n",
    "out.to_csv(dataSaveDIR+'grid_S_I.csv'\n",
    "           ,index=False,header=out.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random select\n",
    "sampS = sample(range(len(acs6.index)),1000) \n",
    "\n",
    "sampS = np.array(sampS)\n",
    "np.savetxt(dataSaveDIR+'sampS.txt',sampS)\n",
    "\n",
    "# make training data for counterfactual locations\n",
    "\n",
    "grid_S_random_I = [] # Random points to the location of 200 restaurants\n",
    "grid_S_random_S = [] # Random points to the location of all real retail stores\n",
    "\n",
    "ety2 = ety.copy(deep=True)\n",
    "\n",
    "cnt=1000 \n",
    "\n",
    "for i in sampS:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    lat = acs6.loc[i,'LATITUDE']\n",
    "    lon = acs6.loc[i,'LONGITUDE']\n",
    "    temp = acs6[acs6['LATITUDE']>=lat-M_delta_degrees2].copy() \n",
    "    temp = temp[temp['LATITUDE']<=lat+M_delta_degrees2]\n",
    "    temp = temp[temp['LONGITUDE']>=lon-M_delta_degrees2]\n",
    "    temp = temp[temp['LONGITUDE']<=lon+M_delta_degrees2]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'LATITUDE'],temp.loc[j,'LONGITUDE'])\n",
    "        line = [cnt,temp.loc[j,'No'],xt,yt,temp.loc[j,'population']\n",
    "                                                        ,temp.loc[j,'share_age_under16']\n",
    "                                                        ,temp.loc[j,'share_age_16_64']\n",
    "                                                        ,temp.loc[j,'share_white']\n",
    "                                                        ,temp.loc[j,'share_college']\n",
    "                                                        ,temp.loc[j,'share_collegeplus']\n",
    "                                                        ,temp.loc[j,'median_hhinc']\n",
    "                                                        ,temp.loc[j,'share_unemployed']\n",
    "                                                        ,temp.loc[j,'median_hh_value']\n",
    "                                                        ,temp.loc[j,'share_trans_car']\n",
    "                                                        ,temp.loc[j,'share_trans_public']\n",
    "                                                        ,temp.loc[j,'share_trans_home']\n",
    "                                                        ,temp.loc[j,'share_travel_0_15']\n",
    "                                                        ,temp.loc[j,'share_travel_15_30']\n",
    "                                                        ,temp.loc[j,'share_travel_30_45']\n",
    "                                                        ,temp.loc[j,'share_travel_45_60']\n",
    "                                                        ,temp.loc[j,'share_travel_60_90']\n",
    "                                                        ,temp.loc[j,'cbg_id']\n",
    "                                                        ,temp.loc[j,'minorRace']]\n",
    "        grid_S_random_I.append(line)\n",
    "        \n",
    "        \n",
    "    temp = ety2[ety2[etylat]>=lat-M_delta_degrees3].copy()\n",
    "    temp = temp[temp[etylat]<=lat+M_delta_degrees3]\n",
    "    temp = temp[temp[etylon]>=lon-M_delta_degrees3]\n",
    "    temp = temp[temp[etylon]<=lon+M_delta_degrees3]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,etylat],temp.loc[j,etylon])\n",
    "        line = [cnt,temp.loc[j,'No'],xt,yt]\n",
    "        grid_S_random_S.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_random_I)\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_oth','x','y','population', 'share_age_under16', 'share_age_16_64',\n",
    "       'share_white', 'share_college', 'share_collegeplus', 'median_hhinc',\n",
    "       'share_unemployed', 'median_hh_value', 'share_trans_car',\n",
    "       'share_trans_public', 'share_trans_home', 'share_travel_0_15',\n",
    "       'share_travel_15_30', 'share_travel_30_45', 'share_travel_45_60',\n",
    "       'share_travel_60_90', 'cbg_id', 'minorRace'])\n",
    "\n",
    "temp2 = np.array(grid_S_random_S)\n",
    "out2 = pd.DataFrame(data=temp2, index=None, columns=['S_id','S_id_oth','x','y'])\n",
    "\n",
    "# save data\n",
    "out.to_csv(dataSaveDIR+'grid_S_random_I.csv'\n",
    "           ,index=False,header=out.columns,sep=',')\n",
    "out2.to_csv(dataSaveDIR+'grid_S_random_S.csv'\n",
    "           ,index=False,header=out2.columns,sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Industry Distribution Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in data\n",
    "poi1 = pd.read_csv(dataInDIR + POIfileName)\n",
    "# number of POIs\n",
    "poi1 = poi1.drop_duplicates(subset=['placekey'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "poi2 = [] # Random points to the location of 200 restaurants\n",
    "\n",
    "#cnt=1000\n",
    "for i in ety.index:\n",
    "    print(i)\n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "    temp = poi1[poi1['lat']>=lat-p_delta_degrees].copy() \n",
    "    temp = temp[temp['lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['lon']<=lon+p_delta_degrees]\n",
    "    for j in temp.index:\n",
    "        line = [temp.loc[j,'placekey'],temp.loc[j,'start_date'],temp.loc[j,'end_date'],\n",
    "                float(temp.loc[j,'naics']),float(temp.loc[j,'lon']),float(temp.loc[j,'lat']),\n",
    "                float(ety.loc[i,'No']), float(ety.loc[i,etylat]),float(ety.loc[i,etylon]), \n",
    "                ety.loc[i,etyopendate]\n",
    "                ]  \n",
    "\n",
    "        poi2.append(line)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = poi2.copy()\n",
    "poi3 = pd.DataFrame(data=temp, index=None, columns=['poi_placekey', 'start_date', 'end_date', 'poi_naics',\n",
    "       'poi_lon', 'poi_lat', 'entry_number', 'entry_opening_lat', 'entry_opening_lon', 'entry_opening_date'\n",
    "       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 5 Catogories + 1 Competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding start and end day for POIs (in datetime dataform)\n",
    "\n",
    "poi4 = poi3.copy(deep=True)\n",
    "startdate = []\n",
    "endate = []\n",
    "\n",
    "for i in range(len(poi3.index)):\n",
    "    \n",
    "    temp = poi4['start_date'][i]\n",
    "    temp1 = datetime.datetime.strptime(temp,'%Y-%m-%d')\n",
    "    startdate.append(temp1)\n",
    "    \n",
    "    temp = poi4['end_date'][i]\n",
    "    temp1 = datetime.datetime.strptime(temp,'%Y-%m-%d')\n",
    "    endate.append(temp1)\n",
    "    \n",
    "poi4['poiStartDate'] = startdate\n",
    "poi4['poiEndDate'] = endate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding opening datetime for entry openings\n",
    "poi5 = poi4.copy(deep=True)\n",
    "entryMon = poi5['entry_opening_date'].copy()\n",
    "\n",
    "poi5['entryOpenDate'] = entryMon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter entry openings according to opening month\n",
    "poi6 = poi5.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# five categories\n",
    "poi7 = poi6.copy(deep=True)\n",
    "shortNaics = poi7['poi_naics'].copy()\n",
    "shortNaics = np.array([float(item) for item in shortNaics])\n",
    "shortNaics = np.floor(shortNaics/10000)\n",
    "\n",
    "poi7['shortPoiNaics'] = shortNaics\n",
    "\n",
    "# category 1\n",
    "cate1 = poi7[poi7['shortPoiNaics']>=52].copy(deep=True)\n",
    "cate1 = cate1[cate1['shortPoiNaics']<=56]\n",
    "\n",
    "#cate1\n",
    "\n",
    "# category 2\n",
    "cate2 = poi7[poi7['shortPoiNaics']==62].copy(deep=True)\n",
    "\n",
    "# category 3\n",
    "cate3 = poi7[poi7['shortPoiNaics']==72].copy(deep=True)\n",
    "\n",
    "# category 4\n",
    "cate41 = poi7[poi7['shortPoiNaics']==42].copy(deep=True)\n",
    "cate42 = poi7[poi7['shortPoiNaics']==44].copy(deep=True)\n",
    "cate43 = poi7[poi7['shortPoiNaics']==45].copy(deep=True)\n",
    "\n",
    "cate4 = cate41.append(cate42).copy()\n",
    "cate4 = cate4.append(cate43)\n",
    "\n",
    "# category 5\n",
    "cate51 = poi7[poi7['shortPoiNaics']==51].copy(deep=True)\n",
    "cate52 = poi7[poi7['shortPoiNaics']==61].copy(deep=True)\n",
    "cate53 = poi7[poi7['shortPoiNaics']==71].copy(deep=True)\n",
    "cate54 = poi7[poi7['shortPoiNaics']==81].copy(deep=True)\n",
    "cate55 = poi7[poi7['shortPoiNaics']==92].copy(deep=True)\n",
    "\n",
    "cate5 = cate51.append(cate52).copy()\n",
    "cate5 = cate5.append(cate53)\n",
    "cate5 = cate5.append(cate54)\n",
    "cate5 = cate5.append(cate55)\n",
    "\n",
    "cate1.to_csv(dataSaveDIR+'F3cate1.csv',\n",
    "           index=False,header=cate1.columns,sep=',')\n",
    "cate2.to_csv(dataSaveDIR+'F3cate2.csv',\n",
    "           index=False,header=cate2.columns,sep=',')\n",
    "cate3.to_csv(dataSaveDIR+'F3cate3.csv',\n",
    "           index=False,header=cate3.columns,sep=',')\n",
    "cate4.to_csv(dataSaveDIR+'F3cate4.csv',\n",
    "           index=False,header=cate4.columns,sep=',')\n",
    "cate5.to_csv(dataSaveDIR+'F3cate5.csv',\n",
    "           index=False,header=cate5.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# competitors\n",
    "poi8 = poi6.copy(deep=True)\n",
    "sixDNaics = poi8['poi_naics'].copy()\n",
    "sixDNaics = np.array([float(item) for item in sixDNaics])\n",
    "sixDNaics = np.floor(sixDNaics)\n",
    "\n",
    "poi8['6digitPoiNaics'] = sixDNaics\n",
    "\n",
    "comp1 = poi8[poi8['6digitPoiNaics']==445110].copy(deep=True)\n",
    "comp2 = poi8[poi8['6digitPoiNaics']==445120].copy(deep=True)\n",
    "comp3 = poi8[poi8['6digitPoiNaics']==452210].copy(deep=True)\n",
    "comp4 = poi8[poi8['6digitPoiNaics']==452319].copy(deep=True)\n",
    "\n",
    "comp = comp1.append(comp2).copy()\n",
    "comp = comp.append(comp3)\n",
    "comp = comp.append(comp4)\n",
    "\n",
    "comp.to_csv(dataSaveDIR+'F3comp.csv',\n",
    "           index=False,header=comp.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constucting inputs\n",
    "grid_S_I_cate1 = []\n",
    "for i in range(len(ety.index)):\n",
    "    print(i)\n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "    temp = cate1[cate1['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [ety.loc[i,'No'],j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_I_cate1.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_I_cate1)\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate1','x','y','naics'])\n",
    "\n",
    "# save data\n",
    "out.to_csv(dataSaveDIR+'grid_S_I_cate1.csv'\n",
    "           ,index=False,header=out.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_S_I_cate2 = []\n",
    "for i in range(len(ety.index)):\n",
    "    print(i)\n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "    temp = cate2[cate2['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [ety.loc[i,'No'],j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_I_cate2.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_I_cate2)\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate2','x','y','naics'])\n",
    "\n",
    "# save data\n",
    "out.to_csv(dataSaveDIR+'grid_S_I_cate2.csv'\n",
    "           ,index=False,header=out.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_S_I_cate3 = []\n",
    "for i in range(len(ety.index)):\n",
    "    print(i)\n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "    temp = cate3[cate3['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [ety.loc[i,'No'],j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_I_cate3.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_I_cate3)\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate3','x','y','naics'])\n",
    "\n",
    "# save data\n",
    "out.to_csv(dataSaveDIR+'grid_S_I_cate3.csv'\n",
    "           ,index=False,header=out.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_S_I_cate4 = []\n",
    "for i in range(len(ety.index)):\n",
    "    print(i)\n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "    temp = cate4[cate4['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [ety.loc[i,'No'],j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_I_cate4.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_I_cate4)\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate4','x','y','naics'])\n",
    "\n",
    "# save data\n",
    "out.to_csv(dataSaveDIR+'grid_S_I_cate4.csv'\n",
    "           ,index=False,header=out.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_S_I_cate5 = []\n",
    "for i in range(len(ety.index)):\n",
    "    print(i)\n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "    temp = cate5[cate5['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [ety.loc[i,'No'],j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_I_cate5.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_I_cate5)\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate5','x','y','naics'])\n",
    "\n",
    "# save data\n",
    "out.to_csv(dataSaveDIR+'grid_S_I_cate5.csv'\n",
    "           ,index=False,header=out.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comp\n",
    "grid_S_I_comp = []\n",
    "for i in range(len(ety.index)):\n",
    "    print(i)\n",
    "    lat = ety.loc[i,etylat]\n",
    "    lon = ety.loc[i,etylon]\n",
    "    temp = comp[comp['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [ety.loc[i,'No'],j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_I_comp.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_I_comp)\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_comp','x','y','naics'])\n",
    "\n",
    "# save data\n",
    "out.to_csv(dataSaveDIR+'grid_S_I_comp.csv'\n",
    "           ,index=False,header=out.columns,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Counterfactual Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for S_random\n",
    "\n",
    "pr_delta_radious = 5/mile_R # Michael's criteria for raw data, within 5 miles parameter!!!\n",
    "pr_delta_degrees = math.degrees(pr_delta_radious)\n",
    "\n",
    "poi2r = [] # Random points to the location of 200 restaurants\n",
    "\n",
    "#cnt=1000\n",
    "for i in sampS:\n",
    "    lat = acs6.loc[i,'LATITUDE']\n",
    "    lon = acs6.loc[i,'LONGITUDE']\n",
    "    temp = poi1[poi1['lat']>=lat-pr_delta_degrees].copy() \n",
    "    temp = temp[temp['lat']<=lat+pr_delta_degrees]\n",
    "    temp = temp[temp['lon']>=lon-pr_delta_degrees]\n",
    "    temp = temp[temp['lon']<=lon+pr_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        line = [temp.loc[j,'placekey'],temp.loc[j,'start_date'],temp.loc[j,'end_date'],\n",
    "                float(temp.loc[j,'naics']),float(temp.loc[j,'lon']),float(temp.loc[j,'lat']),\n",
    "                float(acs6.loc[i,'No']),lat,lon,\n",
    "                ]\n",
    "        poi2r.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = poi2r.copy()\n",
    "poi3r = pd.DataFrame(data=temp, index=None, columns=['poi_placekey', 'start_date', 'end_date', 'poi_naics',\n",
    "       'poi_lon', 'poi_lat', 'entry_number', 'entry_opening_lat', 'entry_opening_lon',\n",
    "                                                    ])\n",
    "poi3r = poi3r.drop_duplicates(subset=['poi_placekey'])\n",
    "poi3r.index = range(len(poi3r.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding start and end day for POIs (in datetime dataform)\n",
    "\n",
    "poi4r = poi3r.copy(deep=True)\n",
    "startdate = []\n",
    "endate = []\n",
    "\n",
    "for i in range(len(poi3r.index)):\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    temp = poi4r['start_date'][i]\n",
    "    temp1 = datetime.datetime.strptime(temp,'%Y-%m-%d')\n",
    "    startdate.append(temp1)\n",
    "    \n",
    "    temp = poi4r['end_date'][i]\n",
    "    temp1 = datetime.datetime.strptime(temp,'%Y-%m-%d')\n",
    "    endate.append(temp1)\n",
    "    \n",
    "poi4r['poiStartDate'] = startdate\n",
    "poi4r['poiEndDate'] = endate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding opening datetime for entry openings\n",
    "poi5r = poi4r.copy(deep=True)\n",
    "etyMon = []\n",
    "\n",
    "for item in range(len(poi4r.index)):\n",
    "\n",
    "    temp = datetime.datetime(2019, 12, 1, 0, 0, 0)\n",
    "\n",
    "    etyMon.append(temp)\n",
    "    \n",
    "poi5r['entryOpenMon'] = etyMon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi6r = poi3r.copy(deep=True)\n",
    "\n",
    "# five categories\n",
    "poi7r = poi6r.copy(deep=True)\n",
    "shortNaics = poi7r['poi_naics'].copy()\n",
    "shortNaics = np.array([float(item) for item in shortNaics])\n",
    "shortNaics = np.floor(shortNaics/10000)\n",
    "#shortNaics = np.int(shortNaics)\n",
    "#shortNaics\n",
    "poi7r['shortPoiNaics'] = shortNaics\n",
    "\n",
    "# category 1\n",
    "cate1 = poi7r[poi7r['shortPoiNaics']>=52].copy(deep=True)\n",
    "cate1 = cate1[cate1['shortPoiNaics']<=56]\n",
    "\n",
    "#cate1\n",
    "\n",
    "# category 2\n",
    "cate2 = poi7r[poi7r['shortPoiNaics']==62].copy(deep=True)\n",
    "\n",
    "# category 3\n",
    "cate3 = poi7r[poi7r['shortPoiNaics']==72].copy(deep=True)\n",
    "\n",
    "# category 4\n",
    "cate41 = poi7r[poi7r['shortPoiNaics']==42].copy(deep=True)\n",
    "cate42 = poi7r[poi7r['shortPoiNaics']==44].copy(deep=True)\n",
    "cate43 = poi7r[poi7r['shortPoiNaics']==45].copy(deep=True)\n",
    "\n",
    "cate4 = cate41.append(cate42).copy()\n",
    "cate4 = cate4.append(cate43)\n",
    "\n",
    "# category 5\n",
    "cate51 = poi7r[poi7r['shortPoiNaics']==51].copy(deep=True)\n",
    "cate52 = poi7r[poi7r['shortPoiNaics']==61].copy(deep=True)\n",
    "cate53 = poi7r[poi7r['shortPoiNaics']==71].copy(deep=True)\n",
    "cate54 = poi7r[poi7r['shortPoiNaics']==81].copy(deep=True)\n",
    "cate55 = poi7r[poi7r['shortPoiNaics']==92].copy(deep=True)\n",
    "\n",
    "cate5 = cate51.append(cate52).copy()\n",
    "cate5 = cate5.append(cate53)\n",
    "cate5 = cate5.append(cate54)\n",
    "cate5 = cate5.append(cate55)\n",
    "\n",
    "cate1.to_csv(dataSaveDIR+'F3cate1r.csv',\n",
    "           index=False,header=cate1.columns,sep=',')\n",
    "cate2.to_csv(dataSaveDIR+'F3cate2r.csv',\n",
    "           index=False,header=cate2.columns,sep=',')\n",
    "cate3.to_csv(dataSaveDIR+'F3cate3r.csv',\n",
    "           index=False,header=cate3.columns,sep=',')\n",
    "cate4.to_csv(dataSaveDIR+'F3cate4r.csv',\n",
    "           index=False,header=cate4.columns,sep=',')\n",
    "cate5.to_csv(dataSaveDIR+'F3cate5r.csv',\n",
    "           index=False,header=cate5.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# competitors\n",
    "poi8r = poi6r.copy(deep=True)\n",
    "sixDNaics = poi8r['poi_naics'].copy()\n",
    "sixDNaics = np.array([float(item) for item in sixDNaics])\n",
    "sixDNaics = np.floor(sixDNaics)\n",
    "\n",
    "poi8r['6digitPoiNaics'] = sixDNaics\n",
    "\n",
    "comp1 = poi8r[poi8r['6digitPoiNaics']==445110].copy(deep=True)\n",
    "comp2 = poi8r[poi8r['6digitPoiNaics']==445120].copy(deep=True)\n",
    "comp3 = poi8r[poi8r['6digitPoiNaics']==452210].copy(deep=True)\n",
    "comp4 = poi8r[poi8r['6digitPoiNaics']==452319].copy(deep=True)\n",
    "\n",
    "comp = comp1.append(comp2).copy()\n",
    "comp = comp.append(comp3)\n",
    "comp = comp.append(comp4)\n",
    "\n",
    "comp.to_csv(dataSaveDIR+'F3compr.csv',\n",
    "           index=False,header=comp.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_S_random_I_cate1 = [] #\n",
    "cnt=1000 \n",
    "for i in sampS: \n",
    "    cnt+=1\n",
    "    lat = acs6.loc[i,'LATITUDE']\n",
    "    lon = acs6.loc[i,'LONGITUDE']\n",
    "\n",
    "    temp = cate1[cate1['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [cnt,j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_random_I_cate1.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_random_I_cate1) #\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate1','x','y','naics'])\n",
    "\n",
    "# save data\n",
    "out.to_csv(dataSaveDIR+'grid_S_random_I_cate1.csv' #\n",
    "           ,index=False,header=out.columns,sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cntr = 1000\n",
    "\n",
    "grid_S_random_I_cate2 = [] #\n",
    "cnt=cntr\n",
    "for i in sampS: \n",
    "    cnt+=1\n",
    "    lat = acs6.loc[i,'LATITUDE']\n",
    "    lon = acs6.loc[i,'LONGITUDE']\n",
    "\n",
    "    temp = cate2[cate2['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [cnt,j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_random_I_cate2.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_random_I_cate2) #\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate2','x','y','naics'])\n",
    "\n",
    "# save data\n",
    "out.to_csv(dataSaveDIR+'grid_S_random_I_cate2.csv' #\n",
    "           ,index=False,header=out.columns,sep=',')\n",
    "\n",
    "grid_S_random_I_cate3 = [] #\n",
    "cnt=cntr\n",
    "for i in sampS: \n",
    "    cnt+=1\n",
    "    lat = acs6.loc[i,'LATITUDE']\n",
    "    lon = acs6.loc[i,'LONGITUDE']\n",
    "\n",
    "    temp = cate3[cate3['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [cnt,j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_random_I_cate3.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_random_I_cate3) #\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate3','x','y','naics'])\n",
    "\n",
    "out.to_csv(dataSaveDIR+'grid_S_random_I_cate3.csv' #\n",
    "           ,index=False,header=out.columns,sep=',')\n",
    "\n",
    "grid_S_random_I_cate4 = [] #\n",
    "cnt=cntr\n",
    "for i in sampS:\n",
    "    cnt+=1\n",
    "    lat = acs6.loc[i,'LATITUDE']\n",
    "    lon = acs6.loc[i,'LONGITUDE']\n",
    "\n",
    "    temp = cate4[cate4['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [cnt,j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_random_I_cate4.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_random_I_cate4) #\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate4','x','y','naics'])\n",
    "\n",
    "out.to_csv(dataSaveDIR+'grid_S_random_I_cate4.csv' #\n",
    "           ,index=False,header=out.columns,sep=',')\n",
    "\n",
    "grid_S_random_I_cate5 = [] #\n",
    "cnt=cntr\n",
    "for i in sampS: \n",
    "    cnt+=1\n",
    "    lat = acs6.loc[i,'LATITUDE']\n",
    "    lon = acs6.loc[i,'LONGITUDE']\n",
    "\n",
    "    temp = cate5[cate5['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [cnt,j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_random_I_cate5.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_random_I_cate5) #\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_cate5','x','y','naics'])\n",
    "\n",
    "out.to_csv(dataSaveDIR+'grid_S_random_I_cate5.csv' #\n",
    "           ,index=False,header=out.columns,sep=',')\n",
    "\n",
    "grid_S_random_I_comp = [] #\n",
    "cnt=cntr\n",
    "for i in sampS: \n",
    "    cnt+=1\n",
    "    lat = acs6.loc[i,'LATITUDE']\n",
    "    lon = acs6.loc[i,'LONGITUDE']\n",
    "\n",
    "    temp = comp[comp['poi_lat']>=lat-p_delta_degrees].copy()\n",
    "    temp = temp[temp['poi_lat']<=lat+p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']>=lon-p_delta_degrees]\n",
    "    temp = temp[temp['poi_lon']<=lon+p_delta_degrees]\n",
    "    #samp = sample(range(nbct), 200)\n",
    "    for j in temp.index:\n",
    "        xt,yt = Loc(lat,lon,temp.loc[j,'poi_lat'],temp.loc[j,'poi_lon'])\n",
    "        line = [cnt,j,xt,yt,temp.loc[j,'poi_naics']]\n",
    "        grid_S_random_I_comp.append(line)\n",
    "        \n",
    "temp = np.array(grid_S_random_I_comp) #\n",
    "out = pd.DataFrame(data=temp, index=None, columns=['S_id','S_id_comp','x','y','naics'])\n",
    "\n",
    "out.to_csv(dataSaveDIR+'grid_S_random_I_comp.csv' #\n",
    "           ,index=False,header=out.columns,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:try3]",
   "language": "python",
   "name": "conda-env-try3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
