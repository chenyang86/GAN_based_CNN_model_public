{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74KpXDvt8zN-"
   },
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "eVhVjQ2F8k0R"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math as mt\n",
    "from datetime import datetime, timedelta\n",
    "from math import sin, cos, pi  # rotating regions\n",
    "from math import floor  # truncating naics codes\n",
    "import numba  # speed up data transform with JIT compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tSA42ig814T"
   },
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "CgmUJGEX83r6"
   },
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = # where you save and load data\n",
    "\n",
    "# \"grid coordinates\" created in R are in meters\n",
    "cell_width = 1 * 0.025 * 1.609344 * 1000  # cell width in meters (convert from miles)\n",
    "size_potential = 10  # potential locations: num_width_potential x num_width_potential\n",
    "size_padding = 20  # number of padding cells on each side of potential grid\n",
    "nc = 25 #3  # number of channels\n",
    "\n",
    "BATCH_SIZE_real = 96  # regions with missing grocery store per batch #96\n",
    "BATCH_SIZE_fill = 16 #16  # regions with real location filled in (-> no missing) per batch\n",
    "BATCH_SIZE_random = 8  # random regions (-> no missing) per batch\n",
    "BATCH_SIZE = BATCH_SIZE_real + BATCH_SIZE_fill + BATCH_SIZE_random\n",
    "\n",
    "frac_train_real = 1.0  # fraction of real regions to use for training\n",
    "frac_train_random = 1.0  # fraction of random (unrealized) regions to use for training\n",
    "\n",
    "use_saved_model = False #True\n",
    "saved_model_filename = # filename if choose to use saved model\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "EPOCHS = 10\n",
    "sampleTotNum = 418\n",
    "ITERS = 100 \n",
    "\n",
    "labelThresh = 1000 # start from where there are counterfactual openings\n",
    "\n",
    "weight_fine = 1\n",
    "weight_plain = 1\n",
    "\n",
    "eval_weight_fine = 1\n",
    "eval_weight_plain = 1\n",
    "\n",
    "fineSampleNumber = 418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHIdQSa9m6MN",
    "outputId": "2dc65281-8e5f-461a-868a-915b6d421b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE: 120\n",
      "cell_width: 40m\n"
     ]
    }
   ],
   "source": [
    "print('BATCH_SIZE: ' + str(BATCH_SIZE))\n",
    "print('cell_width: ' + str(round(cell_width)) + 'm')\n",
    "# print('prob_none: ' + str(round(100 * prob_none)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKwhhVl5-0UC"
   },
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "1lT1xAWg9SPq"
   },
   "outputs": [],
   "source": [
    "dict_S_I = dict()\n",
    "dict_S_I_restaurant = dict()\n",
    "\n",
    "# read in data of businesses near each grocery store\n",
    "with open(dataroot+'grid_S_I.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        i_id = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "\n",
    "        if slist[4] == 'NA':\n",
    "            popu = -1\n",
    "        else:\n",
    "            popu = float(slist[4])\n",
    "\n",
    "        if slist[5] == 'NA':\n",
    "            sau16 = -1\n",
    "        else:\n",
    "            sau16 = float(slist[5])\n",
    "\n",
    "        if slist[6] == 'NA':\n",
    "            sa1664 = -1\n",
    "        else:\n",
    "            sa1664 = float(slist[6])\n",
    "                    \n",
    "        if slist[7] == 'NA':\n",
    "            sw = -1\n",
    "        else:\n",
    "            sw = float(slist[7])\n",
    "                    \n",
    "        if slist[8] == 'NA':\n",
    "            sc = -1\n",
    "        else:\n",
    "            sc = float(slist[8])\n",
    "                    \n",
    "        if slist[9] == 'NA':\n",
    "            scp = -1\n",
    "        else:\n",
    "            scp = float(slist[9])\n",
    "                    \n",
    "        if slist[10] == 'NA':\n",
    "            mhh = -1\n",
    "        else:\n",
    "            mhh = float(slist[10])\n",
    "                    \n",
    "        if slist[11] == 'NA':\n",
    "            suem = -1\n",
    "        else:\n",
    "            suem = float(slist[11])\n",
    "                    \n",
    "        if slist[12] == 'NA':\n",
    "            mhhv = -1\n",
    "        else:\n",
    "            mhhv = float(slist[12])\n",
    "                    \n",
    "        if slist[13] == 'NA':\n",
    "            stc = -1\n",
    "        else:\n",
    "            stc = float(slist[13])\n",
    "                    \n",
    "        if slist[14] == 'NA':\n",
    "            stp = -1\n",
    "        else:\n",
    "            stp = float(slist[14])\n",
    "                    \n",
    "        if slist[15] == 'NA':\n",
    "            sth = -1\n",
    "        else:\n",
    "            sth = float(slist[15])\n",
    "                    \n",
    "        if slist[16] == 'NA':\n",
    "            st15 = -1\n",
    "        else:\n",
    "            st15 = float(slist[16])\n",
    "                    \n",
    "        if slist[17] == 'NA':\n",
    "            st30 = -1\n",
    "        else:\n",
    "            st30 = float(slist[17])\n",
    "                    \n",
    "        if slist[18] == 'NA':\n",
    "            st45 = -1\n",
    "        else:\n",
    "            st45 = float(slist[18])\n",
    "                    \n",
    "        if slist[19] == 'NA':\n",
    "            st60 = -1\n",
    "        else:\n",
    "            st60 = float(slist[19])\n",
    "                    \n",
    "        if slist[20] == 'NA':\n",
    "            st90 = -1\n",
    "        else:\n",
    "            st90 = float(slist[20])\n",
    "                    \n",
    "        if slist[22] == 'NA':\n",
    "            minr = -1\n",
    "        else:\n",
    "            minr = float(slist[22])\n",
    "        \n",
    "        tup = (x,y,popu, sau16, sa1664, sw, sc, scp, mhh, suem, \n",
    "               mhhv, stc, stp, sth, st15, st30, st45, st60, st90, minr)\n",
    "        \n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I.keys():\n",
    "            dict_S_I[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I[s_id].append(tup)\n",
    "\n",
    "# read in data of businesses near each grocery store\n",
    "with open(dataroot+'grid_S_random_I.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        i_id = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "\n",
    "        if slist[4] == 'NA':\n",
    "            popu = -1\n",
    "        else:\n",
    "            popu = float(slist[4])\n",
    "\n",
    "        if slist[5] == 'NA':\n",
    "            sau16 = -1\n",
    "        else:\n",
    "            sau16 = float(slist[5])\n",
    "\n",
    "        if slist[6] == 'NA':\n",
    "            sa1664 = -1\n",
    "        else:\n",
    "            sa1664 = float(slist[6])\n",
    "                    \n",
    "        if slist[7] == 'NA':\n",
    "            sw = -1\n",
    "        else:\n",
    "            sw = float(slist[7])\n",
    "                    \n",
    "        if slist[8] == 'NA':\n",
    "            sc = -1\n",
    "        else:\n",
    "            sc = float(slist[8])\n",
    "                    \n",
    "        if slist[9] == 'NA':\n",
    "            scp = -1\n",
    "        else:\n",
    "            scp = float(slist[9])\n",
    "                    \n",
    "        if slist[10] == 'NA':\n",
    "            mhh = -1\n",
    "        else:\n",
    "            mhh = float(slist[10])\n",
    "                    \n",
    "        if slist[11] == 'NA':\n",
    "            suem = -1\n",
    "        else:\n",
    "            suem = float(slist[11])\n",
    "                    \n",
    "        if slist[12] == 'NA':\n",
    "            mhhv = -1\n",
    "        else:\n",
    "            mhhv = float(slist[12])\n",
    "                    \n",
    "        if slist[13] == 'NA':\n",
    "            stc = -1\n",
    "        else:\n",
    "            stc = float(slist[13])\n",
    "                    \n",
    "        if slist[14] == 'NA':\n",
    "            stp = -1\n",
    "        else:\n",
    "            stp = float(slist[14])\n",
    "                    \n",
    "        if slist[15] == 'NA':\n",
    "            sth = -1\n",
    "        else:\n",
    "            sth = float(slist[15])\n",
    "                    \n",
    "        if slist[16] == 'NA':\n",
    "            st15 = -1\n",
    "        else:\n",
    "            st15 = float(slist[16])\n",
    "                    \n",
    "        if slist[17] == 'NA':\n",
    "            st30 = -1\n",
    "        else:\n",
    "            st30 = float(slist[17])\n",
    "                    \n",
    "        if slist[18] == 'NA':\n",
    "            st45 = -1\n",
    "        else:\n",
    "            st45 = float(slist[18])\n",
    "                    \n",
    "        if slist[19] == 'NA':\n",
    "            st60 = -1\n",
    "        else:\n",
    "            st60 = float(slist[19])\n",
    "                    \n",
    "        if slist[20] == 'NA':\n",
    "            st90 = -1\n",
    "        else:\n",
    "            st90 = float(slist[20])\n",
    "                    \n",
    "        #if slist[21] == 'NA':\n",
    "        #    naics = -1\n",
    "        #else:\n",
    "        #    naics = int(float(slist[21]))\n",
    "                    \n",
    "        if slist[22] == 'NA':\n",
    "            minr = -1\n",
    "        else:\n",
    "            minr = float(slist[22])\n",
    "        \n",
    "        tup = (x,y,popu, sau16, sa1664, sw, sc, scp, mhh, suem, \n",
    "               mhhv, stc, stp, sth, st15, st30, st45, st60, st90, minr)\n",
    "\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I.keys():\n",
    "            dict_S_I[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I[s_id].append(tup)\n",
    "\n",
    "dict_S_I_cate1 = dict()\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_I_cate1.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate1.keys():\n",
    "            dict_S_I_cate1[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate1[s_id].append(tup)\n",
    "\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_random_I_cate1.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate1.keys():\n",
    "            dict_S_I_cate1[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate1[s_id].append(tup)\n",
    "\n",
    "#dict_S_S = dict()\n",
    "dict_S_I_cate2 = dict()\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_I_cate2.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate2.keys():\n",
    "            dict_S_I_cate2[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate2[s_id].append(tup)\n",
    "\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_random_I_cate2.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate2.keys():\n",
    "            dict_S_I_cate2[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate2[s_id].append(tup)\n",
    "\n",
    "#dict_S_S = dict()\n",
    "dict_S_I_cate3 = dict()\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_I_cate3.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate3.keys():\n",
    "            dict_S_I_cate3[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate3[s_id].append(tup)\n",
    "\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_random_I_cate3.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate3.keys():\n",
    "            dict_S_I_cate3[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate3[s_id].append(tup)\n",
    "\n",
    "#dict_S_S = dict()\n",
    "dict_S_I_cate4 = dict()\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_I_cate4.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate4.keys():\n",
    "            dict_S_I_cate4[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate4[s_id].append(tup)\n",
    "\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_random_I_cate4.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate4.keys():\n",
    "            dict_S_I_cate4[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate4[s_id].append(tup)\n",
    "\n",
    "dict_S_I_cate5 = dict()\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_I_cate5.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate5.keys():\n",
    "            dict_S_I_cate5[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate5[s_id].append(tup)\n",
    "\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_random_I_cate5.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_cate5.keys():\n",
    "            dict_S_I_cate5[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_cate5[s_id].append(tup)\n",
    "\n",
    "dict_S_I_comp = dict()\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_I_comp.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_comp.keys():\n",
    "            dict_S_I_comp[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_comp[s_id].append(tup)\n",
    "\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_random_I_comp.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_cate1 = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_I_comp.keys():\n",
    "            dict_S_I_comp[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_I_comp[s_id].append(tup)\n",
    "\n",
    "dict_S_I_mat = dict()\n",
    "for key, value in dict_S_I.items():\n",
    "   \n",
    "    # create a numpy array\n",
    "    mat = np.empty((len(value), 20)) # 20 = 2 + 18\n",
    "    for i in range(len(value)):\n",
    "        #print(i)\n",
    "        mat[i,0] = value[i][0]\n",
    "        mat[i,1] = value[i][1]\n",
    "        mat[i,2] = value[i][2]\n",
    "        mat[i,3] = value[i][3]\n",
    "        mat[i,4] = value[i][4]\n",
    "        #print(value[i][4])\n",
    "        mat[i,5] = value[i][5]\n",
    "        mat[i,6] = value[i][6]\n",
    "        mat[i,7] = value[i][7]\n",
    "        mat[i,8] = value[i][8]\n",
    "        mat[i,9] = value[i][9]\n",
    "        mat[i,10] = value[i][10]\n",
    "        mat[i,11] = value[i][11]\n",
    "        mat[i,12] = value[i][12]\n",
    "        mat[i,13] = value[i][13]\n",
    "        mat[i,14] = value[i][14]\n",
    "        mat[i,15] = value[i][15]\n",
    "        mat[i,16] = value[i][16]\n",
    "        mat[i,17] = value[i][17]\n",
    "        mat[i,18] = value[i][18]\n",
    "        mat[i,19] = value[i][19]\n",
    "        \n",
    "    dict_S_I_mat[key] = mat\n",
    "    \n",
    "dict_S_S = dict()\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_S.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_oth = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_S.keys():\n",
    "            dict_S_S[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_S[s_id].append(tup)\n",
    "\n",
    "# read in data of grocery stores near each grocery store\n",
    "with open(dataroot+'grid_S_random_S.csv','r') as f:\n",
    "    for line in f:\n",
    "        # skip header\n",
    "        if line.startswith('S_id'):\n",
    "            continue\n",
    "        # extract data\n",
    "        slist = line.strip().split(',')\n",
    "        s_id = int(float(slist[0]))\n",
    "        s_id_oth = int(float(slist[1]))\n",
    "        x = float(slist[2])\n",
    "        y = float(slist[3])\n",
    "        tup = (x,y)\n",
    "        # create entry if first time we encounter s_id\n",
    "        if not s_id in dict_S_S.keys():\n",
    "            dict_S_S[s_id] = list()\n",
    "        # add data to this s_id\n",
    "        dict_S_S[s_id].append(tup)\n",
    "\n",
    "dict_S_S_mat = dict()\n",
    "for key, value in dict_S_S.items():\n",
    "    # create a numpy array\n",
    "    mat = np.empty((len(value), 2))\n",
    "    for i in range(len(value)):\n",
    "        mat[i,0] = value[i][0]\n",
    "        mat[i,1] = value[i][1]\n",
    "    dict_S_S_mat[key] = mat\n",
    "# regions without grocery stores:\n",
    "for key in set(dict_S_I.keys()) - set(dict_S_S.keys()):\n",
    "    mat = np.empty((0,2))\n",
    "    dict_S_S_mat[key] = mat\n",
    "    \n",
    "dict_S_I_cate1_mat = dict()\n",
    "for key, value in dict_S_I_cate1.items():\n",
    "    # create a numpy array\n",
    "    mat = np.empty((len(value), 2))\n",
    "    for i in range(len(value)):\n",
    "        mat[i,0] = value[i][0]\n",
    "        mat[i,1] = value[i][1]\n",
    "    dict_S_I_cate1_mat[key] = mat\n",
    "# regions without grocery stores:\n",
    "for key in set(dict_S_I.keys()) - set(dict_S_I_cate1.keys()):\n",
    "    mat = np.empty((0,2))\n",
    "    dict_S_I_cate1_mat[key] = mat\n",
    "\n",
    "dict_S_I_cate2_mat = dict()\n",
    "for key, value in dict_S_I_cate2.items():\n",
    "    # create a numpy array\n",
    "    mat = np.empty((len(value), 2))\n",
    "    for i in range(len(value)):\n",
    "        mat[i,0] = value[i][0]\n",
    "        mat[i,1] = value[i][1]\n",
    "    dict_S_I_cate2_mat[key] = mat\n",
    "# regions without grocery stores:\n",
    "for key in set(dict_S_I.keys()) - set(dict_S_I_cate2.keys()):\n",
    "    mat = np.empty((0,2))\n",
    "    dict_S_I_cate2_mat[key] = mat\n",
    "\n",
    "dict_S_I_cate3_mat = dict()\n",
    "for key, value in dict_S_I_cate3.items():\n",
    "    # create a numpy array\n",
    "    mat = np.empty((len(value), 2))\n",
    "    for i in range(len(value)):\n",
    "        mat[i,0] = value[i][0]\n",
    "        mat[i,1] = value[i][1]\n",
    "    dict_S_I_cate3_mat[key] = mat\n",
    "# regions without grocery stores:\n",
    "for key in set(dict_S_I.keys()) - set(dict_S_I_cate3.keys()):\n",
    "    mat = np.empty((0,2))\n",
    "    dict_S_I_cate3_mat[key] = mat\n",
    "\n",
    "dict_S_I_cate4_mat = dict()\n",
    "for key, value in dict_S_I_cate4.items():\n",
    "    # create a numpy array\n",
    "    mat = np.empty((len(value), 2))\n",
    "    for i in range(len(value)):\n",
    "        mat[i,0] = value[i][0]\n",
    "        mat[i,1] = value[i][1]\n",
    "    dict_S_I_cate4_mat[key] = mat\n",
    "# regions without grocery stores:\n",
    "for key in set(dict_S_I.keys()) - set(dict_S_I_cate4.keys()):\n",
    "    mat = np.empty((0,2))\n",
    "    dict_S_I_cate4_mat[key] = mat\n",
    "\n",
    "dict_S_I_cate5_mat = dict()\n",
    "for key, value in dict_S_I_cate5.items():\n",
    "    # create a numpy array\n",
    "    mat = np.empty((len(value), 2))\n",
    "    for i in range(len(value)):\n",
    "        mat[i,0] = value[i][0]\n",
    "        mat[i,1] = value[i][1]\n",
    "    dict_S_I_cate5_mat[key] = mat\n",
    "# regions without grocery stores:\n",
    "for key in set(dict_S_I.keys()) - set(dict_S_I_cate5.keys()):\n",
    "    mat = np.empty((0,2))\n",
    "    dict_S_I_cate5_mat[key] = mat\n",
    "\n",
    "dict_S_I_comp_mat = dict()\n",
    "for key, value in dict_S_I_comp.items():\n",
    "    # create a numpy array\n",
    "    mat = np.empty((len(value), 2))\n",
    "    for i in range(len(value)):\n",
    "        mat[i,0] = value[i][0]\n",
    "        mat[i,1] = value[i][1]\n",
    "    dict_S_I_comp_mat[key] = mat\n",
    "    \n",
    "# regions without grocery stores:\n",
    "for key in set(dict_S_I.keys()) - set(dict_S_I_comp.keys()):\n",
    "    mat = np.empty((0,2))\n",
    "    dict_S_I_comp_mat[key] = mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_id_all = list(dict_S_I.keys())\n",
    "# print(S_id_all)\n",
    "S_id_real = [el for el in S_id_all if el<labelThresh]\n",
    "# print(S_id_real)\n",
    "S_id_random = [el for el in S_id_all if el>=labelThresh]\n",
    "# print(S_id_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tizQNABK9aTp"
   },
   "source": [
    "## Transform Data to grid (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "id": "fAhOphzJ9iyz"
   },
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def cnt_in_cell_mat(x,y):\n",
    "    out = np.zeros((2*size_padding+size_potential,2*size_padding+size_potential))\n",
    "    for i in range(len(x)):\n",
    "        if min(x[i],y[i])>= 0 and max(x[i],y[i])<2*size_padding+size_potential:\n",
    "            out[y[i],x[i]] += 1\n",
    "    return out\n",
    "\n",
    "def cnt_in_cell_mat_value(x,y,value):\n",
    "    out = np.zeros((2*size_padding+size_potential,2*size_padding+size_potential))\n",
    "    for i in range(len(x)):\n",
    "        if min(x[i],y[i])>= 0 and max(x[i],y[i])<2*size_padding+size_potential:\n",
    "            out[y[i],x[i]] += value[i]\n",
    "    return out\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def data_shift_rotate(mat,shift_x=0,shift_y=0,theta=0,mirror_var=1):\n",
    "    # rotate by theta\n",
    "    theta = theta * pi / 180\n",
    "    if not theta == 0:\n",
    "        x = cos(theta) * mat[:,0] - sin(theta) * mat[:,1]\n",
    "        y = sin(theta) * mat[:,0] + cos(theta) * mat[:,1]\n",
    "\n",
    "    else:\n",
    "        x = mat[:,0]\n",
    "        y = mat[:,1]\n",
    "    # mirror the region\n",
    "    if mirror_var == 1 or mirror_var == -1:\n",
    "        x = mirror_var * x\n",
    "    # shift by shift_x, shift_y\n",
    "    x = x+shift_x\n",
    "    y = y+shift_y\n",
    "\n",
    "    return x, y  \n",
    "\n",
    "def data_to_grid(mat,shift_x=0,shift_y=0,theta=0,mirror_var=1):\n",
    "\n",
    "    x, y = data_shift_rotate(mat,shift_x,shift_y,theta,mirror_var)\n",
    "\n",
    "    # fit into cells\n",
    "    x = np.around(x/cell_width + size_padding).astype(int)\n",
    "    y = np.around(y/cell_width + size_padding).astype(int)\n",
    "\n",
    "    return cnt_in_cell_mat(x,y)\n",
    "\n",
    "def data_to_grid_value(mat,shift_x=0,shift_y=0,theta=0,mirror_var=1,valueNo=2):\n",
    "\n",
    "    x, y = data_shift_rotate(mat,shift_x,shift_y,theta,mirror_var)\n",
    "    #print(x,y)\n",
    "    value = mat[:,valueNo]\n",
    "    #print(value)\n",
    "\n",
    "    # fit into cells\n",
    "    x = np.around(x/cell_width + size_padding).astype(int)\n",
    "    y = np.around(y/cell_width + size_padding).astype(int)\n",
    "    #print(x,y)\n",
    "\n",
    "    return cnt_in_cell_mat_value(x,y,value)\n",
    "\n",
    "def create_matLabel(y,x, gratitude = 0.15):\n",
    "\n",
    "    if gratitude==0:\n",
    "        out = np.zeros([size_potential,size_potential])\n",
    "        out[y,x] = 1\n",
    "\n",
    "        for yi in range(size_potential):\n",
    "            for xi in range(size_potential):\n",
    "                out[yi,xi] = 1 * mt.exp(- 1.0*((yi-y)*(yi-y)+(xi-x)*(xi-x)))\n",
    "        out = np.append(out,0)\n",
    "    elif gratitude==1:\n",
    "        out = np.zeros(100)\n",
    "        out = np.append(out,1)\n",
    "\n",
    "    return out.flatten()\n",
    "\n",
    "# create tensor of the proper size (1 channel currently)\n",
    "grid = torch.zeros(BATCH_SIZE,nc,2*size_padding+size_potential, 2*size_padding+size_potential) #, dtype=torch.double)\n",
    "labels = torch.empty(BATCH_SIZE, dtype=torch.int64)\n",
    "mat_labels = torch.empty(BATCH_SIZE, size_potential*size_potential+1, dtype=torch.float)\n",
    "\n",
    "#def create_batch(grid=grid,labels=labels,sample_ids_real=S_id_real, real_ids_weight = None, sample_ids_random=S_id_random, return_transf=False):\n",
    "def create_batch(grid=grid,mat_labels=mat_labels, pos_label=pos_label,\n",
    "                 logit_label=logit_label, sample_ids_real=S_id_real, real_ids_weight = None, \n",
    "                 sample_ids_random=S_id_random, return_transf=False, trainbool = 0):\n",
    "\n",
    "    grid = grid*0\n",
    "\n",
    "    mat_labels = mat_labels*0\n",
    "    \n",
    "    if return_transf:\n",
    "        transf = np.zeros(shape=(BATCH_SIZE,5))\n",
    "\n",
    "    for b in range(BATCH_SIZE):\n",
    "\n",
    "        if b < BATCH_SIZE_real + BATCH_SIZE_fill:\n",
    "            s_id = np.random.choice(sample_ids_real, p = real_ids_weight, replace=True)\n",
    "        else:\n",
    "            s_id = np.random.choice(sample_ids_random)\n",
    "        #print(s_id)\n",
    "        # get the businesses near s_id\n",
    "        mat_S = dict_S_S_mat[s_id]\n",
    "        mat_I = dict_S_I_mat[s_id]\n",
    "        #mat_I_restaurant = dict_S_I_restaurant_mat[s_id]  \n",
    "        mat_I_cate1 = dict_S_I_cate1_mat[s_id]\n",
    "        mat_I_cate2 = dict_S_I_cate2_mat[s_id]\n",
    "        mat_I_cate3 = dict_S_I_cate3_mat[s_id]\n",
    "        mat_I_cate4 = dict_S_I_cate4_mat[s_id]\n",
    "        mat_I_cate5 = dict_S_I_cate5_mat[s_id]\n",
    "        mat_I_comp = dict_S_I_comp_mat[s_id]\n",
    "\n",
    "        if trainbool==0:\n",
    "            # randomly pick rotation of this region\n",
    "            theta = np.random.rand()*360\n",
    "\n",
    "            # randomly mirror? \n",
    "            mirror_var = (np.random.rand() > 0.5)*2 - 1\n",
    "                #print(mirror_var)\n",
    "            # randomly pick where real store is going to be\n",
    "            shift_x = np.random.rand()*cell_width*size_potential - cell_width/2\n",
    "            shift_y = np.random.rand()*cell_width*size_potential - cell_width/2\n",
    "            \n",
    "        elif trainbool==1:\n",
    "            # randomly pick rotation of this region\n",
    "            theta = 0\n",
    "\n",
    "            # randomly mirror? \n",
    "            mirror_var = 1\n",
    "                #print(mirror_var)\n",
    "            # randomly pick where real store is going to be\n",
    "            shift_x = np.random.rand()*cell_width*size_potential - cell_width/2\n",
    "            shift_y = np.random.rand()*cell_width*size_potential - cell_width/2\n",
    "            \n",
    "\n",
    "        # print(shift_x,shift_y,theta,mirror_var)\n",
    "        if return_transf:\n",
    "            transf[b,0] = s_id\n",
    "            transf[b,1] = shift_x\n",
    "            transf[b,2] = shift_y\n",
    "            transf[b,3] = theta\n",
    "            transf[b,4] = mirror_var\n",
    "\n",
    "        # fill tensor\n",
    "        grid[b,0,:,:] = torch.from_numpy(data_to_grid(mat_S,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var))\n",
    "        if nc >= 2:\n",
    "            grid[b,1,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=2))\n",
    "            grid[b,2,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=3))\n",
    "            grid[b,3,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=4))\n",
    "            grid[b,4,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=5))\n",
    "            grid[b,5,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=6))\n",
    "            grid[b,6,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=7))\n",
    "            grid[b,7,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=8))\n",
    "            grid[b,8,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=9))\n",
    "            grid[b,9,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=10))\n",
    "            grid[b,10,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=11))\n",
    "            grid[b,11,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=12))\n",
    "            grid[b,12,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=13))\n",
    "            grid[b,13,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=14))\n",
    "            grid[b,14,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=15))\n",
    "            grid[b,15,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=16))\n",
    "            grid[b,16,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=17))\n",
    "            grid[b,17,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=18))\n",
    "            grid[b,18,:,:] = torch.from_numpy(data_to_grid_value(mat_I,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var,valueNo=19))\n",
    "\n",
    "        if nc >= 20:\n",
    "            grid[b,19,:,:] = torch.from_numpy(data_to_grid(mat_I_cate1,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var))\n",
    "            grid[b,20,:,:] = torch.from_numpy(data_to_grid(mat_I_cate2,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var))\n",
    "            grid[b,21,:,:] = torch.from_numpy(data_to_grid(mat_I_cate3,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var))\n",
    "            grid[b,22,:,:] = torch.from_numpy(data_to_grid(mat_I_cate4,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var))\n",
    "            grid[b,23,:,:] = torch.from_numpy(data_to_grid(mat_I_cate5,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var))\n",
    "\n",
    "        if nc >= 25:\n",
    "            grid[b,24,:,:] = torch.from_numpy(data_to_grid(mat_I_comp,shift_x=shift_x,shift_y=shift_y,theta=theta,mirror_var=mirror_var))\n",
    "\n",
    "\n",
    "        # include this grocery store in the covariates? (whether to gauge the point out)\n",
    "\n",
    "        if b <= BATCH_SIZE_real:\n",
    "            treat_x = int(round(shift_x/cell_width) + size_padding)\n",
    "            treat_y = int(round(shift_y/cell_width) + size_padding)\n",
    "            grid[b,0,treat_y,treat_x] -= 1\n",
    "        \n",
    "\n",
    "        # location of missing grocery store:\n",
    "        if b <= BATCH_SIZE_real:\n",
    "            mat_labels[b,:] = torch.from_numpy(create_matLabel(int(round(shift_y/cell_width)), int(round(shift_x/cell_width)),0))\n",
    "\n",
    "        # random region without missing grocery store or grocery store is filled in:\n",
    "        else:\n",
    "            mat_labels[b,:] = torch.from_numpy(create_matLabel(int(round(shift_y/cell_width)), int(round(shift_x/cell_width)), 1))\n",
    "            \n",
    "    if not return_transf:\n",
    "        return grid, mat_labels #logit_label, pos_label\n",
    "    else:\n",
    "        return grid, mat_labels, transf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEvGrZ3_9AIN"
   },
   "source": [
    "## Define Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "id": "6LaVyy4X9JMh"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            nn.InstanceNorm2d(num_features=nc, affine=True),\n",
    "            nn.Conv2d(in_channels=nc,\n",
    "                out_channels=2*nc,\n",
    "                kernel_size=5, #9,\n",
    "                padding=2, #4, #(9-1)/2,\n",
    "                padding_mode='replicate', # 'zeros', 'reflect' or 'replicate' could work\n",
    "                bias=True),\n",
    "            nn.InstanceNorm2d(num_features=2*nc, affine=True),\n",
    "            nn.LeakyReLU(),\n",
    "           \n",
    "            nn.Conv2d(in_channels=2*nc,\n",
    "                out_channels=4*nc,\n",
    "                kernel_size=21, # 21 #9,\n",
    "                padding=20,#20  #4, #(9-1)/2,\n",
    "                padding_mode='replicate', # 'zeros', 'reflect' or 'replicate' could work\n",
    "                dilation=2,\n",
    "                bias=True),\n",
    "            nn.InstanceNorm2d(num_features=4*nc, affine=True),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=4*nc,\n",
    "                out_channels=4*nc,\n",
    "                kernel_size=5, #9,\n",
    "                padding=2, #4, #(9-1)/2,\n",
    "                padding_mode='replicate', # 'zeros', 'reflect' or 'replicate' could work\n",
    "                bias=True),\n",
    "            nn.InstanceNorm2d(num_features=4*nc, affine=True),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=4*nc,\n",
    "                out_channels=1, #1\n",
    "                kernel_size=21, #9,\n",
    "                padding=20, #4, #(9-1)/2,\n",
    "                padding_mode='replicate', # 'zeros', 'reflect' or 'replicate' could work\n",
    "                dilation=2,\n",
    "                bias=True),\n",
    "\n",
    "            nn.InstanceNorm2d(num_features=1, affine=True), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(1*pow(2*size_padding+size_potential,2), pow(size_potential,2)+1),  #1\n",
    "        )\n",
    "\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(inputs)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFwEwV9yXh5x"
   },
   "source": [
    "## Initialize optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "id": "kM5CjuB6XhlH"
   },
   "outputs": [],
   "source": [
    "def intitialize_optimizer(net):\n",
    "    #return optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    #return optim.AdamW(net.parameters(), lr=0.001)\n",
    "    return optim.Adam(net.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8eMRusBVmhY"
   },
   "source": [
    "## Code for saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "id": "7L8I5rsHVmOZ"
   },
   "outputs": [],
   "source": [
    "def save_model(filename=None):\n",
    "    if not filename:\n",
    "        date = (datetime.utcnow() + timedelta(hours=-7)).strftime('%Y-%m-%d--%H-%M')\n",
    "        filename = 'classifier-checkpoint-' + date + '.tar'\n",
    "    path_save = dataroot + filename\n",
    "    # save the model\n",
    "    torch.save({\n",
    "                # 'epoch': epoch,\n",
    "                'net_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, path_save)\n",
    "    print('file: ' + path_save)\n",
    "\n",
    "def load_model(filename,net=None,optimizer=None):\n",
    "    path_load = dataroot + filename\n",
    "    if not net:\n",
    "        net = Net()\n",
    "    # if using GPU\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    if not optimizer:\n",
    "        optimizer = intitialize_optimizer(net)\n",
    "    checkpoint = torch.load(path_load)\n",
    "    net.load_state_dict(checkpoint['net_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return net, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KyBZ10x86H2"
   },
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCnTtzGI88X9",
    "outputId": "d4d666e7-4fa0-4c9b-be60-2319ac3fa4b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  24601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d0dca917f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 24601\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvSSqqmCArx9"
   },
   "source": [
    "## Pick training / evaluation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "id": "NDlcRx_6Aun3"
   },
   "outputs": [],
   "source": [
    "# locations for training and evaluation\n",
    "\n",
    "num_distinct_train_real = int(len(S_id_real) * frac_train_real)\n",
    "num_distinct_train_random = int(len(S_id_random) * frac_train_random)\n",
    "\n",
    "#TODO: implement such that eval regions are non-overlapping with train regions\n",
    "SidRealWeight = []\n",
    "for i in S_id_real:\n",
    "    if i<=(fineSampleNumber-1):\n",
    "        SidRealWeight.append(weight_fine)\n",
    "    else:\n",
    "        SidRealWeight.append(weight_plain)\n",
    "        \n",
    "SidRealWeight = np.array(SidRealWeight)\n",
    "SidRealWeight = SidRealWeight/np.sum(SidRealWeight)\n",
    "\n",
    "sample_train_real = list(np.random.choice(a=S_id_real, size=num_distinct_train_real, replace=True, p = SidRealWeight))\n",
    "sample_train_real.sort()\n",
    "\n",
    "\n",
    "sample_train_random = list(np.random.choice(a=S_id_random,size=num_distinct_train_random,replace=False))\n",
    "sample_train_random.sort()\n",
    "\n",
    "if num_distinct_train_real < len(S_id_real):\n",
    "    sample_eval_real = list(set(S_id_real) - set(sample_train_real))\n",
    "else:\n",
    "    sample_eval_real = S_id_real\n",
    "sample_eval_real.sort()\n",
    "\n",
    "final_eval_real = [item for item in S_id_real if item<= (fineSampleNumber-1)]\n",
    "\n",
    "# weights\n",
    "train_id_weight = []\n",
    "for i in sample_train_real:\n",
    "    if i<=(fineSampleNumber-1):\n",
    "        train_id_weight.append(weight_fine)\n",
    "    else:\n",
    "        train_id_weight.append(weight_plain)\n",
    "        \n",
    "train_id_weight = np.array(train_id_weight)\n",
    "train_id_weight = train_id_weight/np.sum(train_id_weight)\n",
    "\n",
    "eval_id_weight = []\n",
    "for i in sample_eval_real:\n",
    "    if i<=(fineSampleNumber-1):\n",
    "        eval_id_weight.append(eval_weight_fine)\n",
    "    else:\n",
    "        eval_id_weight.append(eval_weight_plain)\n",
    "        \n",
    "eval_id_weight = np.array(eval_id_weight)\n",
    "eval_id_weight = eval_id_weight/np.sum(eval_id_weight)\n",
    "        \n",
    "finalEval_id_weight = []\n",
    "for i in final_eval_real:\n",
    "    if i<=(fineSampleNumber-1):\n",
    "        finalEval_id_weight.append(eval_weight_fine)\n",
    "    else:\n",
    "        finalEval_id_weight.append(eval_weight_plain)\n",
    "        \n",
    "finalEval_id_weight = np.array(finalEval_id_weight)\n",
    "finalEval_id_weight = finalEval_id_weight/np.sum(finalEval_id_weight)\n",
    "        \n",
    "\n",
    "if num_distinct_train_random < len(S_id_random):\n",
    "    sample_eval_random = list(set(S_id_random) - set(sample_train_random))\n",
    "else:\n",
    "    sample_eval_random = S_id_random\n",
    "sample_eval_random.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lylewd1b9nl5"
   },
   "source": [
    "## Initialize neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "id": "gEiahC669qC9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        #print('h')\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "if use_saved_model:\n",
    "    net, optimizer = load_model(saved_model_filename)\n",
    "else:\n",
    "    net = Net()\n",
    "    net.apply(weights_init)\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    optimizer = intitialize_optimizer(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRDyO8509sAF"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "g6sG1ZO59wF7",
    "outputId": "228a0565-82da-4a3a-baf5-dec4d6f97dfa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "2022-08-31 00:38:57\n",
      "epoch: 10 iteration: 100\n",
      "[1 / 10,    10 /   100] loss: 11.733, accuracy: 6.8%, real: 1.6%, real proxy: 10.7%, real non-zero: 76.2%, real filled: 28.1%, unrealized: 26.2%\n",
      "[1 / 10,    20 /   100] loss: 10.471, accuracy: 11.2%, real: 8.4%, real proxy: 27.0%, real non-zero: 75.5%, real filled: 28.8%, unrealized: 10.0%\n",
      "[1 / 10,    30 /   100] loss: 8.540, accuracy: 30.8%, real: 34.1%, real proxy: 61.6%, real non-zero: 85.8%, real filled: 17.5%, unrealized: 17.5%\n",
      "[1 / 10,    40 /   100] loss: 7.166, accuracy: 48.8%, real: 55.6%, real proxy: 80.9%, real non-zero: 92.7%, real filled: 19.4%, unrealized: 26.2%\n",
      "[1 / 10,    50 /   100] loss: 6.521, accuracy: 58.7%, real: 60.9%, real proxy: 83.6%, real non-zero: 94.2%, real filled: 51.9%, unrealized: 45.0%\n",
      "[1 / 10,    60 /   100] loss: 6.087, accuracy: 69.8%, real: 65.5%, real proxy: 87.3%, real non-zero: 96.0%, real filled: 95.6%, unrealized: 68.8%\n",
      "[1 / 10,    70 /   100] loss: 5.992, accuracy: 71.0%, real: 65.6%, real proxy: 87.1%, real non-zero: 97.3%, real filled: 98.8%, unrealized: 80.0%\n",
      "[1 / 10,    80 /   100] loss: 5.780, accuracy: 72.9%, real: 69.2%, real proxy: 89.5%, real non-zero: 97.7%, real filled: 97.5%, unrealized: 68.8%\n",
      "[1 / 10,    90 /   100] loss: 5.747, accuracy: 72.8%, real: 67.7%, real proxy: 89.4%, real non-zero: 98.3%, real filled: 98.1%, unrealized: 82.5%\n",
      "[1 / 10,   100 /   100] loss: 5.687, accuracy: 72.7%, real: 68.0%, real proxy: 88.4%, real non-zero: 97.7%, real filled: 98.1%, unrealized: 77.5%\n",
      "Finished Epoch 1 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--21-46.tar\n",
      "2022-08-30 21:46:18\n",
      "[2 / 10,    10 /   100] loss: 5.698, accuracy: 74.9%, real: 70.9%, real proxy: 90.0%, real non-zero: 98.6%, real filled: 96.2%, unrealized: 80.0%\n",
      "[2 / 10,    20 /   100] loss: 5.596, accuracy: 76.7%, real: 71.8%, real proxy: 90.9%, real non-zero: 97.8%, real filled: 100.0%, unrealized: 88.8%\n",
      "[2 / 10,    30 /   100] loss: 5.549, accuracy: 77.6%, real: 73.1%, real proxy: 91.5%, real non-zero: 98.6%, real filled: 98.1%, unrealized: 90.0%\n",
      "[2 / 10,    40 /   100] loss: 5.604, accuracy: 77.6%, real: 72.7%, real proxy: 91.0%, real non-zero: 99.1%, real filled: 98.1%, unrealized: 95.0%\n",
      "[2 / 10,    50 /   100] loss: 5.510, accuracy: 79.8%, real: 75.5%, real proxy: 92.1%, real non-zero: 99.0%, real filled: 97.5%, unrealized: 95.0%\n",
      "[2 / 10,    60 /   100] loss: 5.602, accuracy: 75.9%, real: 70.9%, real proxy: 89.3%, real non-zero: 98.1%, real filled: 99.4%, unrealized: 88.8%\n",
      "[2 / 10,    70 /   100] loss: 5.439, accuracy: 80.2%, real: 76.4%, real proxy: 91.7%, real non-zero: 99.0%, real filled: 96.2%, unrealized: 93.8%\n",
      "[2 / 10,    80 /   100] loss: 5.520, accuracy: 78.9%, real: 75.3%, real proxy: 92.5%, real non-zero: 98.8%, real filled: 96.2%, unrealized: 87.5%\n",
      "[2 / 10,    90 /   100] loss: 5.575, accuracy: 78.0%, real: 73.8%, real proxy: 91.4%, real non-zero: 97.9%, real filled: 98.1%, unrealized: 88.8%\n",
      "[2 / 10,   100 /   100] loss: 5.463, accuracy: 80.2%, real: 76.2%, real proxy: 91.6%, real non-zero: 98.1%, real filled: 98.1%, unrealized: 91.2%\n",
      "Finished Epoch 2 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--21-53.tar\n",
      "2022-08-30 21:53:40\n",
      "[3 / 10,    10 /   100] loss: 5.398, accuracy: 81.8%, real: 78.4%, real proxy: 91.7%, real non-zero: 98.2%, real filled: 96.9%, unrealized: 92.5%\n",
      "[3 / 10,    20 /   100] loss: 5.411, accuracy: 81.5%, real: 78.1%, real proxy: 92.3%, real non-zero: 98.8%, real filled: 97.5%, unrealized: 90.0%\n",
      "[3 / 10,    30 /   100] loss: 5.439, accuracy: 79.8%, real: 75.7%, real proxy: 92.1%, real non-zero: 99.8%, real filled: 98.1%, unrealized: 91.2%\n",
      "[3 / 10,    40 /   100] loss: 5.405, accuracy: 81.3%, real: 77.6%, real proxy: 92.2%, real non-zero: 99.1%, real filled: 97.5%, unrealized: 93.8%\n",
      "[3 / 10,    50 /   100] loss: 5.366, accuracy: 81.5%, real: 77.6%, real proxy: 91.4%, real non-zero: 98.3%, real filled: 99.4%, unrealized: 92.5%\n",
      "[3 / 10,    60 /   100] loss: 5.370, accuracy: 83.2%, real: 79.7%, real proxy: 93.5%, real non-zero: 99.6%, real filled: 99.4%, unrealized: 93.8%\n",
      "[3 / 10,    70 /   100] loss: 5.342, accuracy: 82.4%, real: 79.0%, real proxy: 91.7%, real non-zero: 99.2%, real filled: 99.4%, unrealized: 90.0%\n",
      "[3 / 10,    80 /   100] loss: 5.379, accuracy: 81.7%, real: 77.7%, real proxy: 92.8%, real non-zero: 98.8%, real filled: 98.8%, unrealized: 95.0%\n",
      "[3 / 10,    90 /   100] loss: 5.274, accuracy: 83.5%, real: 80.0%, real proxy: 92.0%, real non-zero: 99.0%, real filled: 98.8%, unrealized: 95.0%\n",
      "[3 / 10,   100 /   100] loss: 5.309, accuracy: 83.6%, real: 80.4%, real proxy: 92.9%, real non-zero: 99.2%, real filled: 98.1%, unrealized: 92.5%\n",
      "Finished Epoch 3 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--22-01.tar\n",
      "2022-08-30 22:01:04\n",
      "[4 / 10,    10 /   100] loss: 5.240, accuracy: 85.2%, real: 82.5%, real proxy: 94.1%, real non-zero: 99.1%, real filled: 97.5%, unrealized: 92.5%\n",
      "[4 / 10,    20 /   100] loss: 5.331, accuracy: 85.8%, real: 83.1%, real proxy: 93.0%, real non-zero: 99.5%, real filled: 97.5%, unrealized: 95.0%\n",
      "[4 / 10,    30 /   100] loss: 5.220, accuracy: 85.7%, real: 82.5%, real proxy: 93.6%, real non-zero: 98.5%, real filled: 99.4%, unrealized: 96.2%\n",
      "[4 / 10,    40 /   100] loss: 5.188, accuracy: 86.3%, real: 83.4%, real proxy: 94.8%, real non-zero: 99.4%, real filled: 100.0%, unrealized: 93.8%\n",
      "[4 / 10,    50 /   100] loss: 5.194, accuracy: 85.4%, real: 82.6%, real proxy: 94.6%, real non-zero: 99.8%, real filled: 98.1%, unrealized: 93.8%\n",
      "[4 / 10,    60 /   100] loss: 5.310, accuracy: 84.0%, real: 80.9%, real proxy: 93.2%, real non-zero: 98.5%, real filled: 99.4%, unrealized: 90.0%\n",
      "[4 / 10,    70 /   100] loss: 5.323, accuracy: 84.6%, real: 81.9%, real proxy: 94.3%, real non-zero: 98.9%, real filled: 96.2%, unrealized: 93.8%\n",
      "[4 / 10,    80 /   100] loss: 5.160, accuracy: 87.8%, real: 85.9%, real proxy: 94.3%, real non-zero: 99.2%, real filled: 100.0%, unrealized: 85.0%\n",
      "[4 / 10,    90 /   100] loss: 5.291, accuracy: 85.2%, real: 82.5%, real proxy: 93.4%, real non-zero: 99.3%, real filled: 98.1%, unrealized: 92.5%\n",
      "[4 / 10,   100 /   100] loss: 5.285, accuracy: 85.8%, real: 82.8%, real proxy: 94.0%, real non-zero: 98.9%, real filled: 98.1%, unrealized: 97.5%\n",
      "Finished Epoch 4 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--22-08.tar\n",
      "2022-08-30 22:08:28\n",
      "[5 / 10,    10 /   100] loss: 5.235, accuracy: 84.9%, real: 82.2%, real proxy: 93.0%, real non-zero: 99.1%, real filled: 99.4%, unrealized: 88.8%\n",
      "[5 / 10,    20 /   100] loss: 5.182, accuracy: 87.1%, real: 84.5%, real proxy: 93.9%, real non-zero: 99.3%, real filled: 99.4%, unrealized: 93.8%\n",
      "[5 / 10,    30 /   100] loss: 5.196, accuracy: 87.7%, real: 85.3%, real proxy: 94.9%, real non-zero: 99.5%, real filled: 98.1%, unrealized: 95.0%\n",
      "[5 / 10,    40 /   100] loss: 5.275, accuracy: 86.2%, real: 83.3%, real proxy: 93.9%, real non-zero: 99.5%, real filled: 98.8%, unrealized: 95.0%\n",
      "[5 / 10,    50 /   100] loss: 5.248, accuracy: 85.6%, real: 82.8%, real proxy: 94.1%, real non-zero: 98.9%, real filled: 97.5%, unrealized: 95.0%\n",
      "[5 / 10,    60 /   100] loss: 5.211, accuracy: 85.3%, real: 82.9%, real proxy: 94.6%, real non-zero: 99.9%, real filled: 96.2%, unrealized: 92.5%\n",
      "[5 / 10,    70 /   100] loss: 5.193, accuracy: 87.2%, real: 84.7%, real proxy: 94.5%, real non-zero: 98.9%, real filled: 100.0%, unrealized: 92.5%\n",
      "[5 / 10,    80 /   100] loss: 5.173, accuracy: 85.8%, real: 83.2%, real proxy: 93.3%, real non-zero: 99.3%, real filled: 98.1%, unrealized: 92.5%\n",
      "[5 / 10,    90 /   100] loss: 5.151, accuracy: 86.8%, real: 84.5%, real proxy: 95.2%, real non-zero: 99.3%, real filled: 98.8%, unrealized: 90.0%\n",
      "[5 / 10,   100 /   100] loss: 5.221, accuracy: 85.0%, real: 82.3%, real proxy: 93.9%, real non-zero: 99.0%, real filled: 98.1%, unrealized: 91.2%\n",
      "Finished Epoch 5 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--22-15.tar\n",
      "2022-08-30 22:15:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 / 10,    10 /   100] loss: 5.204, accuracy: 89.1%, real: 86.8%, real proxy: 94.4%, real non-zero: 99.4%, real filled: 98.8%, unrealized: 97.5%\n",
      "[6 / 10,    20 /   100] loss: 5.157, accuracy: 88.1%, real: 85.4%, real proxy: 94.5%, real non-zero: 99.5%, real filled: 100.0%, unrealized: 96.2%\n",
      "[6 / 10,    30 /   100] loss: 5.191, accuracy: 86.9%, real: 84.8%, real proxy: 94.3%, real non-zero: 99.4%, real filled: 98.8%, unrealized: 88.8%\n",
      "[6 / 10,    40 /   100] loss: 5.062, accuracy: 87.8%, real: 85.3%, real proxy: 94.9%, real non-zero: 99.2%, real filled: 99.4%, unrealized: 95.0%\n",
      "[6 / 10,    50 /   100] loss: 5.100, accuracy: 88.9%, real: 86.6%, real proxy: 95.0%, real non-zero: 99.4%, real filled: 99.4%, unrealized: 96.2%\n",
      "[6 / 10,    60 /   100] loss: 5.169, accuracy: 88.7%, real: 86.1%, real proxy: 95.0%, real non-zero: 99.2%, real filled: 99.4%, unrealized: 97.5%\n",
      "[6 / 10,    70 /   100] loss: 5.165, accuracy: 87.9%, real: 86.0%, real proxy: 94.3%, real non-zero: 99.7%, real filled: 98.1%, unrealized: 90.0%\n",
      "[6 / 10,    80 /   100] loss: 5.142, accuracy: 87.7%, real: 85.3%, real proxy: 94.7%, real non-zero: 99.1%, real filled: 98.8%, unrealized: 93.8%\n",
      "[6 / 10,    90 /   100] loss: 5.145, accuracy: 88.2%, real: 86.2%, real proxy: 94.3%, real non-zero: 99.2%, real filled: 100.0%, unrealized: 87.5%\n",
      "[6 / 10,   100 /   100] loss: 5.075, accuracy: 89.2%, real: 87.0%, real proxy: 95.9%, real non-zero: 99.6%, real filled: 99.4%, unrealized: 96.2%\n",
      "Finished Epoch 6 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--22-23.tar\n",
      "2022-08-30 22:23:18\n",
      "[7 / 10,    10 /   100] loss: 5.127, accuracy: 89.0%, real: 87.0%, real proxy: 95.1%, real non-zero: 99.6%, real filled: 99.4%, unrealized: 92.5%\n",
      "[7 / 10,    20 /   100] loss: 5.168, accuracy: 88.4%, real: 86.2%, real proxy: 94.9%, real non-zero: 99.4%, real filled: 98.8%, unrealized: 93.8%\n",
      "[7 / 10,    30 /   100] loss: 5.129, accuracy: 88.8%, real: 86.5%, real proxy: 95.0%, real non-zero: 99.7%, real filled: 98.8%, unrealized: 97.5%\n",
      "[7 / 10,    40 /   100] loss: 5.147, accuracy: 89.2%, real: 87.2%, real proxy: 94.7%, real non-zero: 98.9%, real filled: 98.8%, unrealized: 93.8%\n",
      "[7 / 10,    50 /   100] loss: 5.119, accuracy: 88.0%, real: 86.0%, real proxy: 94.9%, real non-zero: 99.6%, real filled: 98.1%, unrealized: 91.2%\n",
      "[7 / 10,    60 /   100] loss: 5.054, accuracy: 89.1%, real: 87.0%, real proxy: 95.4%, real non-zero: 99.6%, real filled: 98.8%, unrealized: 95.0%\n",
      "[7 / 10,    70 /   100] loss: 5.087, accuracy: 89.5%, real: 87.8%, real proxy: 94.9%, real non-zero: 99.5%, real filled: 98.8%, unrealized: 91.2%\n",
      "[7 / 10,    80 /   100] loss: 5.096, accuracy: 90.0%, real: 88.1%, real proxy: 94.7%, real non-zero: 99.1%, real filled: 98.8%, unrealized: 95.0%\n",
      "[7 / 10,    90 /   100] loss: 5.174, accuracy: 88.2%, real: 86.1%, real proxy: 95.8%, real non-zero: 99.7%, real filled: 100.0%, unrealized: 90.0%\n",
      "[7 / 10,   100 /   100] loss: 5.078, accuracy: 89.8%, real: 88.3%, real proxy: 95.2%, real non-zero: 99.4%, real filled: 98.1%, unrealized: 91.2%\n",
      "Finished Epoch 7 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--22-30.tar\n",
      "2022-08-30 22:30:44\n",
      "[8 / 10,    10 /   100] loss: 5.129, accuracy: 89.7%, real: 87.8%, real proxy: 95.4%, real non-zero: 99.8%, real filled: 97.5%, unrealized: 96.2%\n",
      "[8 / 10,    20 /   100] loss: 5.102, accuracy: 90.2%, real: 88.1%, real proxy: 94.9%, real non-zero: 99.1%, real filled: 99.4%, unrealized: 97.5%\n",
      "[8 / 10,    30 /   100] loss: 5.181, accuracy: 88.7%, real: 86.8%, real proxy: 94.7%, real non-zero: 99.8%, real filled: 98.8%, unrealized: 91.2%\n",
      "[8 / 10,    40 /   100] loss: 5.126, accuracy: 89.2%, real: 87.3%, real proxy: 94.9%, real non-zero: 98.8%, real filled: 98.8%, unrealized: 93.8%\n",
      "[8 / 10,    50 /   100] loss: 5.055, accuracy: 90.8%, real: 89.1%, real proxy: 95.8%, real non-zero: 99.9%, real filled: 98.8%, unrealized: 96.2%\n",
      "[8 / 10,    60 /   100] loss: 5.111, accuracy: 89.5%, real: 87.8%, real proxy: 95.1%, real non-zero: 99.5%, real filled: 96.9%, unrealized: 95.0%\n",
      "[8 / 10,    70 /   100] loss: 5.084, accuracy: 90.8%, real: 88.9%, real proxy: 96.5%, real non-zero: 99.7%, real filled: 99.4%, unrealized: 96.2%\n",
      "[8 / 10,    80 /   100] loss: 5.066, accuracy: 89.8%, real: 87.7%, real proxy: 95.2%, real non-zero: 99.3%, real filled: 99.4%, unrealized: 95.0%\n",
      "[8 / 10,    90 /   100] loss: 5.023, accuracy: 90.8%, real: 89.7%, real proxy: 96.6%, real non-zero: 99.9%, real filled: 97.5%, unrealized: 91.2%\n",
      "[8 / 10,   100 /   100] loss: 5.087, accuracy: 91.3%, real: 89.5%, real proxy: 95.4%, real non-zero: 99.5%, real filled: 100.0%, unrealized: 96.2%\n",
      "Finished Epoch 8 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--22-38.tar\n",
      "2022-08-30 22:38:08\n",
      "[9 / 10,    10 /   100] loss: 5.144, accuracy: 89.7%, real: 88.1%, real proxy: 94.2%, real non-zero: 99.6%, real filled: 98.1%, unrealized: 91.2%\n",
      "[9 / 10,    20 /   100] loss: 5.139, accuracy: 89.9%, real: 88.3%, real proxy: 96.0%, real non-zero: 99.6%, real filled: 98.8%, unrealized: 91.2%\n",
      "[9 / 10,    30 /   100] loss: 5.092, accuracy: 89.8%, real: 87.8%, real proxy: 95.1%, real non-zero: 99.1%, real filled: 99.4%, unrealized: 93.8%\n",
      "[9 / 10,    40 /   100] loss: 5.127, accuracy: 89.8%, real: 88.1%, real proxy: 95.3%, real non-zero: 99.5%, real filled: 97.5%, unrealized: 93.8%\n",
      "[9 / 10,    50 /   100] loss: 5.020, accuracy: 91.3%, real: 89.6%, real proxy: 95.8%, real non-zero: 99.7%, real filled: 98.1%, unrealized: 98.8%\n",
      "[9 / 10,    60 /   100] loss: 5.054, accuracy: 90.2%, real: 88.8%, real proxy: 95.1%, real non-zero: 99.3%, real filled: 98.1%, unrealized: 92.5%\n",
      "[9 / 10,    70 /   100] loss: 5.047, accuracy: 91.1%, real: 89.5%, real proxy: 95.6%, real non-zero: 99.5%, real filled: 98.1%, unrealized: 96.2%\n",
      "[9 / 10,    80 /   100] loss: 5.100, accuracy: 91.1%, real: 89.3%, real proxy: 95.7%, real non-zero: 99.6%, real filled: 98.8%, unrealized: 97.5%\n",
      "[9 / 10,    90 /   100] loss: 5.057, accuracy: 90.2%, real: 88.2%, real proxy: 95.7%, real non-zero: 99.8%, real filled: 98.1%, unrealized: 98.8%\n",
      "[9 / 10,   100 /   100] loss: 4.993, accuracy: 92.2%, real: 90.8%, real proxy: 96.5%, real non-zero: 99.6%, real filled: 98.8%, unrealized: 96.2%\n",
      "Finished Epoch 9 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--22-45.tar\n",
      "2022-08-30 22:45:33\n",
      "[10 / 10,    10 /   100] loss: 5.036, accuracy: 90.7%, real: 88.9%, real proxy: 96.0%, real non-zero: 99.7%, real filled: 98.8%, unrealized: 96.2%\n",
      "[10 / 10,    20 /   100] loss: 5.029, accuracy: 92.0%, real: 90.2%, real proxy: 96.0%, real non-zero: 99.4%, real filled: 100.0%, unrealized: 97.5%\n",
      "[10 / 10,    30 /   100] loss: 5.011, accuracy: 92.3%, real: 90.6%, real proxy: 97.4%, real non-zero: 99.7%, real filled: 99.4%, unrealized: 98.8%\n",
      "[10 / 10,    40 /   100] loss: 5.042, accuracy: 91.1%, real: 89.2%, real proxy: 95.7%, real non-zero: 99.3%, real filled: 100.0%, unrealized: 96.2%\n",
      "[10 / 10,    50 /   100] loss: 5.039, accuracy: 92.4%, real: 91.1%, real proxy: 96.6%, real non-zero: 99.7%, real filled: 98.1%, unrealized: 96.2%\n",
      "[10 / 10,    60 /   100] loss: 5.025, accuracy: 91.5%, real: 89.6%, real proxy: 96.1%, real non-zero: 99.8%, real filled: 100.0%, unrealized: 97.5%\n",
      "[10 / 10,    70 /   100] loss: 5.070, accuracy: 92.6%, real: 91.1%, real proxy: 95.8%, real non-zero: 99.7%, real filled: 99.4%, unrealized: 96.2%\n",
      "[10 / 10,    80 /   100] loss: 5.043, accuracy: 91.8%, real: 90.3%, real proxy: 96.8%, real non-zero: 99.6%, real filled: 100.0%, unrealized: 92.5%\n",
      "[10 / 10,    90 /   100] loss: 5.012, accuracy: 91.6%, real: 90.2%, real proxy: 96.6%, real non-zero: 99.7%, real filled: 98.8%, unrealized: 93.8%\n",
      "[10 / 10,   100 /   100] loss: 5.051, accuracy: 91.3%, real: 89.9%, real proxy: 95.9%, real non-zero: 99.2%, real filled: 100.0%, unrealized: 91.2%\n",
      "Finished Epoch 10 of 10. Saving model and optimizer checkpoint.\n",
      "file: C:/Users/chensimon/Desktop/SRIntern/Franklin Qian/Amenity/task37 CNN/data/derived220830/classifier-checkpoint-2022-08-30--22-53.tar\n",
      "2022-08-30 22:53:00\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print('Starting Training')\n",
    "print((datetime.utcnow() + timedelta(hours=-4)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# loop over the dataset multiple times\n",
    "print('epoch:',EPOCHS, 'iteration:', ITERS)\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # initialize fit statistics\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    correct_real = 0\n",
    "    non_zero_real = 0\n",
    "    correct_fill = 0\n",
    "    correct_random = 0\n",
    "    total = 0\n",
    "    total_real = 0\n",
    "    total_fill = 0\n",
    "    total_random = 0\n",
    "    cnt_real_about = 0\n",
    "\n",
    "    for i in range(ITERS):\n",
    "        #print(epoch,i)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        data = create_batch(sample_ids_real=sample_train_real, real_ids_weight = train_id_weight,\n",
    "                            sample_ids_random=sample_train_random, trainbool=0)\n",
    "        inputs, labels = data\n",
    "\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # determine accuracy of taking \"prediction\"\n",
    "        with torch.no_grad():\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "\n",
    "            for k in range(len(predicted[0:BATCH_SIZE_real])):\n",
    "                judge = predicted[k]-labels[k]\n",
    "                if judge==1 or judge==-1 or judge==10 or judge==-10 or judge==11 or judge==-11 or judge==9 or judge==-9 or judge==0:\n",
    "                    cnt_real_about+=1\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            correct_real += (predicted[0:BATCH_SIZE_real] == labels[0:BATCH_SIZE_real]).sum().item()\n",
    "            non_zero_real += (predicted[0:BATCH_SIZE_real] < pow(size_potential,2)).sum().item()\n",
    "            correct_fill += (predicted[BATCH_SIZE_real:BATCH_SIZE_real+BATCH_SIZE_fill] == labels[BATCH_SIZE_real:BATCH_SIZE_real+BATCH_SIZE_fill]).sum().item()\n",
    "            correct_random += (predicted[BATCH_SIZE-BATCH_SIZE_random:BATCH_SIZE] == labels[BATCH_SIZE-BATCH_SIZE_random:BATCH_SIZE]).sum().item()\n",
    "            total_real += BATCH_SIZE_real\n",
    "            total_fill += BATCH_SIZE_fill\n",
    "            total_random += BATCH_SIZE_random\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % min(10000,ITERS/10) == min(10000,ITERS/10)-1:    # print every min(1000,ITERS/10) mini-batches\n",
    "            print('[%d / %d, %5d / %5d] loss: %.3f, accuracy: %.1f%%, real: %.1f%%, real proxy: %.1f%%, real non-zero: %.1f%%, real filled: %.1f%%, unrealized: %.1f%%' %\n",
    "                  (epoch + 1, EPOCHS, i + 1, ITERS, running_loss / min(10000,ITERS/10),\n",
    "                   100 * correct / total,\n",
    "                   100 * correct_real / max(total_real,1),\n",
    "                   100 * cnt_real_about / max(total_real,1),\n",
    "                   100 * non_zero_real / max(total_real,1),\n",
    "                   100 * correct_fill / max(total_fill,1),\n",
    "                   100 * correct_random / max(total_random,1)))\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            correct_real = 0\n",
    "            non_zero_real = 0\n",
    "            correct_fill = 0\n",
    "            correct_random = 0\n",
    "            total = 0\n",
    "            total_real = 0\n",
    "            total_fill = 0\n",
    "            total_random = 0\n",
    "            cnt_real_about = 0\n",
    "            \n",
    "        # evaluation sample:\n",
    "        if frac_train_real < 1 or frac_train_random < 1:\n",
    "            if i % min(10000,ITERS/2) == min(10000,ITERS/2)-1:    # print every min(1000,ITERS/10) mini-batches\n",
    "\n",
    "                eval_correct = 0\n",
    "                eval_correct_real = 0\n",
    "                eval_non_zero_real = 0\n",
    "                eval_correct_fill = 0\n",
    "                eval_correct_random = 0\n",
    "                eval_total = 0\n",
    "                eval_total_real = 0\n",
    "                eval_total_fill = 0\n",
    "                eval_total_random = 0\n",
    "                eval_cnt_real_about = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i in range(100):\n",
    "                        inputs, labels = create_batch(sample_ids_real=sample_eval_real, real_ids_weight = eval_id_weight,\n",
    "                                                    sample_ids_random=sample_eval_random, trainbool=1)\n",
    "\n",
    "                        if use_cuda and torch.cuda.is_available():\n",
    "                            inputs = inputs.cuda()\n",
    "                            labels = labels.cuda()\n",
    "\n",
    "                        outputs = net(inputs)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        _, labels = torch.max(labels, 1)\n",
    "                        \n",
    "                        for k in range(len(predicted[0:BATCH_SIZE_real])):\n",
    "                            judge = predicted[k]-labels[k]\n",
    "                            if judge==1 or judge==-1 or judge==10 or judge==-10 or judge==11 or judge==-11 or judge==9 or judge==-9 or judge==0:\n",
    "                                eval_cnt_real_about+=1\n",
    "\n",
    "                        eval_total += labels.size(0)\n",
    "                        eval_correct += (predicted == labels).sum().item()\n",
    "                        eval_correct_real += (predicted[0:BATCH_SIZE_real] == labels[0:BATCH_SIZE_real]).sum().item()\n",
    "                        eval_non_zero_real += (predicted[0:BATCH_SIZE_real] < pow(size_potential,2)).sum().item()\n",
    "                        eval_correct_fill += (predicted[BATCH_SIZE_real:BATCH_SIZE_real+BATCH_SIZE_fill] == labels[BATCH_SIZE_real:BATCH_SIZE_real+BATCH_SIZE_fill]).sum().item()\n",
    "                        eval_correct_random += (predicted[BATCH_SIZE-BATCH_SIZE_random:BATCH_SIZE] == labels[BATCH_SIZE-BATCH_SIZE_random:BATCH_SIZE]).sum().item()\n",
    "                        eval_total_real += BATCH_SIZE_real\n",
    "                        eval_total_fill += BATCH_SIZE_fill\n",
    "                        eval_total_random += BATCH_SIZE_random\n",
    "                        \n",
    "                print('Accuracy on hold-out: %.1f%%, real: %.1f%%, real proxy: %.1f%%, real non-zero: %.1f%%, real filled: %.1f%%, unrealized: %.1f%%' %\n",
    "                    (100 * eval_correct / max(eval_total,1),\n",
    "                    100 * eval_correct_real / max(eval_total_real,1),\n",
    "                    100 * eval_cnt_real_about / max(eval_total_real,1),\n",
    "                    100 * eval_non_zero_real / max(eval_total_real,1),\n",
    "                    100 * eval_correct_fill / max(eval_total_fill,1),\n",
    "                    100 * eval_correct_random / max(eval_total_random,1)))\n",
    "\n",
    "\n",
    "    print('Finished Epoch ' + str(epoch+1) + ' of ' + str(EPOCHS) + '. Saving model and optimizer checkpoint.')\n",
    "    save_model()\n",
    "    print((datetime.utcnow() + timedelta(hours=-7)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkNisLfz97nJ"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Starting Evaluation')\n",
    "\n",
    "eval_correct = 0\n",
    "eval_correct_real = 0\n",
    "eval_non_zero_real = 0\n",
    "eval_correct_fill = 0\n",
    "eval_correct_random = 0\n",
    "eval_total = 0\n",
    "eval_total_real = 0\n",
    "eval_total_fill = 0\n",
    "eval_total_random = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        print(i)\n",
    "        inputs, labels = create_batch(sample_ids_real=sample_eval_real, real_ids_weight = finalEval_id_weight,\n",
    "                                        sample_ids_random=sample_eval_random)\n",
    "\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "\n",
    "        eval_total += labels.size(0)\n",
    "        \n",
    "        eval_correct += (predicted == labels).sum().item()\n",
    "        eval_correct_real += (predicted[0:BATCH_SIZE_real] == labels[0:BATCH_SIZE_real]).sum().item()\n",
    "        eval_non_zero_real += (predicted[0:BATCH_SIZE_real] < pow(size_potential,2)).sum().item()\n",
    "        eval_correct_fill += (predicted[BATCH_SIZE_real:BATCH_SIZE_real+BATCH_SIZE_fill] == labels[BATCH_SIZE_real:BATCH_SIZE_real+BATCH_SIZE_fill]).sum().item()\n",
    "        eval_correct_random += (predicted[BATCH_SIZE-BATCH_SIZE_random:BATCH_SIZE] == labels[BATCH_SIZE-BATCH_SIZE_random:BATCH_SIZE]).sum().item()\n",
    "        \n",
    "        eval_total_real += BATCH_SIZE_real\n",
    "        eval_total_fill += BATCH_SIZE_fill\n",
    "        eval_total_random += BATCH_SIZE_random\n",
    "\n",
    "print('Accuracy on hold-out: %.1f%%, real: %.1f%%, real non-zero: %.1f%%, real filled: %.1f%%, unrealized: %.1f%%' %\n",
    "        (100 * eval_correct / max(eval_total,1),\n",
    "        100 * eval_correct_real / max(eval_total_real,1),\n",
    "        100 * eval_non_zero_real / max(eval_total_real,1),\n",
    "        100 * eval_correct_fill / max(eval_total_fill,1),\n",
    "        100 * eval_correct_random / max(eval_total_random,1)))\n",
    "\n",
    "print((datetime.utcnow() + timedelta(hours=-7)).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sm05u7eRJBYf"
   },
   "source": [
    "## Functions to transform output into list for saving to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "id": "J7ao0qQlR_ao"
   },
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def data_reverse_shift_rotate(xy,shift_x=0,shift_y=0,theta=0,mirror_var=1):\n",
    "    # reverse shift\n",
    "    xy[:,0] -= shift_x\n",
    "    xy[:,1] -= shift_y\n",
    "\n",
    "    # reverse mirroring\n",
    "    if mirror_var == 1 or mirror_var == -1:\n",
    "        xy[:,0] = mirror_var * xy[:,0]\n",
    "    \n",
    "    # reverse rotation by theta\n",
    "    theta = theta * pi / 180\n",
    "    if not theta == 0:\n",
    "        rot = np.array([[cos(theta),sin(theta)],[-sin(theta),cos(theta)]])\n",
    "        xy = xy@np.linalg.inv(rot)\n",
    "    return xy\n",
    "\n",
    "\n",
    "def add_to_list(xy,o,r):\n",
    "    for i in range(len(xy)):\n",
    "        if i == len(xy) - 1:\n",
    "            tup = (str(int(round(xy[i,0]))), str(r), 'NA', 'NA', str(o[i]))\n",
    "        else:\n",
    "            tup = (str(int(round(xy[i,0]))), str(r), str(int(round(xy[i,1]))), str(int(round(xy[i,2]))), str(o[i]))\n",
    "        list_out.append(tup)\n",
    "\n",
    "\n",
    "def outputs_to_loc(outputs,transf):\n",
    "    o = outputs.cpu().numpy()\n",
    "    g = np.linspace(start=cell_width/2,\n",
    "                    stop=cell_width/2 + cell_width*size_potential,\n",
    "                    num=size_potential, endpoint=False)\n",
    "    \n",
    "    for b in range(BATCH_SIZE):\n",
    "        # grid cell midpoints\n",
    "        xy = np.zeros(shape=(pow(size_potential,2)+1,3))\n",
    "        # set s_id\n",
    "        xy[:,0] = int(transf[b,0])\n",
    "        # set relative location\n",
    "        xy[0:pow(size_potential,2),1] = np.tile(g,size_potential)\n",
    "        xy[0:pow(size_potential,2),2] = np.repeat(g,size_potential)\n",
    "        xy[0:pow(size_potential,2),1:3] = data_reverse_shift_rotate(xy[0:pow(size_potential,2),1:3],\n",
    "                                                                    shift_x=transf[b,1],\n",
    "                                                                    shift_y=transf[b,2],\n",
    "                                                                    theta=transf[b,3],\n",
    "                                                                    mirror_var=transf[b,4])\n",
    "        add_to_list(xy,o[b,:],b < BATCH_SIZE_real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHN_G1AZKCBa"
   },
   "source": [
    "## Generate counterfactual locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_out = list()\n",
    "\n",
    "num_batches_predict = 600 # could be more to offer more choices\n",
    "\n",
    "# change ratio of what regions to simulate since we don't care much about the real ones\n",
    "BATCH_SIZE_real = 2  # regions with missing grocery store per batch #2\n",
    "BATCH_SIZE_fill = 2  # regions with real location filled in (-> no missing) per batch #2\n",
    "BATCH_SIZE_random = 28  # random regions (-> no missing) per batch\n",
    "\n",
    "\n",
    "finalEval_id_weight = []\n",
    "for i in final_eval_real:\n",
    "    if i<=int(fineSampleNumber/2-1):\n",
    "        finalEval_id_weight.append(1)\n",
    "    else:\n",
    "        finalEval_id_weight.append(1)\n",
    "\n",
    "        \n",
    "finalEval_id_weight = np.array(finalEval_id_weight)\n",
    "finalEval_id_weight = finalEval_id_weight/np.sum(finalEval_id_weight)        \n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_batches_predict):\n",
    "        print(i)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        data = create_batch(sample_ids_real=final_eval_real,real_ids_weight = finalEval_id_weight, return_transf=True)\n",
    "        inputs, labels, transf = data\n",
    "        #_, labels = torch.max(labels, 1)\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        outputs_to_loc(outputs,transf)\n",
    "\n",
    "# print(list_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rezULOm6Em5D",
    "outputId": "7079812e-abe2-4b12-a0f1-42a8b4b7f790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7272000\n"
     ]
    }
   ],
   "source": [
    "print(len(list_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_14h7VY2M-5c"
   },
   "source": [
    "## Save the resulting file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "id": "nJv3ECtNNBLh"
   },
   "outputs": [],
   "source": [
    "date = (datetime.utcnow() + timedelta(hours=-7)).strftime('%Y-%m-%d--%H-%M')\n",
    "filename_out = 'predicted_activation-' + date + '.csv'\n",
    "with open(dataroot+filename_out,'w') as f:\n",
    "    f.write('S_id,real_missing,x,y,activation\\n')\n",
    "    for e in list_out:\n",
    "        f.write(','.join(e) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Amenity_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:try3]",
   "language": "python",
   "name": "conda-env-try3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
